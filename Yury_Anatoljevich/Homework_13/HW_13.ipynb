{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ДЗ по итогам лекции 16.\n",
    "\n",
    "\"Использовать полученные знания в части обучения моделей для обучения и подбора параметров в задачах из предыдущих ДЗ на выбор (мфо, задача с тем возьмёт ли человек кредит или нет)\n",
    "В частности использовать:\n",
    "\n",
    "GaussianNB\n",
    "\n",
    "LogisticRegression \n",
    "\n",
    "Linear + PolynomialFeatures\n",
    "\n",
    "RandomForestClassifier \n",
    "\n",
    "При обучении использовать make_pipeline\n",
    "\n",
    "GridSearchCV для поиска параметров \n",
    "\n",
    "Посчитать метрики (f1 + точность + AUC). \"\n",
    "\n",
    "\n",
    "Анализ задания и план работы. В части подготовки данных повторяем шаги из предыдущих работ.\n",
    "Следует загрузить файл данных credit_train.csv, представленный в формате csv, в датафрейм модуля pandas.\n",
    "*) Далее следует провести поиск пропусков и удалить строки с пропусками.\n",
    "*) Кроме того, предварительный анализ файла данных показывает, что колонка living region содержит много дублей типа \"московская обл\" и \"обл московская\" или \"чеченская республика\" и \"чеченская респ\". Придется поработать с данными этой колонки, чтобы привести систему обозначений регионов к однозначной. Поскольку регионов слишком много, что затруднит анализ корреляции данных, то мы приведем всю систему к федеральным округам и двум столицам - Москва и Санкт-Петербург.\n",
    "*) Далее для оценки выбросов числовых данных следует провести оценку матожидания и дисперсии каждого столбца данных, представленного в численном виде. Все значения, отстоящие более чем на 3 выборочных стандартных отклонения, удаляем из массива данных (точнее, удаляем из выборки все объекты, для которых хотя бы в одном столбце имеются выбросы).\n",
    "*) С помощью функций подключенной библиотек оценим нормальность распределений данных\n",
    "*) Числовые колонки данных масштабируем, поделив на максимальный элемент каждой колонки\n",
    "\n",
    "\n",
    "Далее, после подготовки данных, выполняем анализ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные.\n",
    "Выводим для ознакомления заголовочную строку и первые пять строк. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in the initial table: 170746\n",
      "Initial column number: 15\n",
      "   client_id gender   age marital_status job_position  credit_sum  \\\n",
      "0          1      M   NaN            NaN          UMN    59998.00   \n",
      "1          2      F   NaN            MAR          UMN    10889.00   \n",
      "2          3      M  32.0            MAR          SPC    10728.00   \n",
      "3          4      F  27.0            NaN          SPC    12009.09   \n",
      "4          5      M  45.0            NaN          SPC         NaN   \n",
      "\n",
      "   credit_month tariff_id  score_shk education        living_region  \\\n",
      "0            10       1.6        NaN       GRD   КРАСНОДАРСКИЙ КРАЙ   \n",
      "1             6       1.1        NaN       NaN               МОСКВА   \n",
      "2            12       1.1        NaN       NaN      ОБЛ САРАТОВСКАЯ   \n",
      "3            12       1.1        NaN       NaN    ОБЛ ВОЛГОГРАДСКАЯ   \n",
      "4            10       1.1   0.421385       SCH  ЧЕЛЯБИНСКАЯ ОБЛАСТЬ   \n",
      "\n",
      "   monthly_income  credit_count  overdue_credit_count  open_account_flg  \n",
      "0         30000.0           1.0                   1.0                 0  \n",
      "1             NaN           2.0                   0.0                 0  \n",
      "2             NaN           5.0                   0.0                 0  \n",
      "3             NaN           2.0                   0.0                 0  \n",
      "4             NaN           1.0                   0.0                 0  \n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "data_table = pd.read_csv('credit_train.csv', sep = ';', decimal = ',', encoding = 'windows-1251')\n",
    "print(f'Rows in the initial table: {len(data_table)}') \n",
    "print(f'Initial column number: {data_table.shape[1]}')\n",
    "print(data_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим сразу много NaN. \n",
    "Это процедура загрузки так заменила пропуски.\n",
    "Такие строки подлежат устранению.\n",
    "Удаляем строки c NaN (то есть с отсутствовавшими изначально значениями, которые при загрузке с помощью функции read_csv превратились в NaN) c помощью dropna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    client_id gender   age marital_status job_position  credit_sum  \\\n",
      "7           8      F  26.0            UNM          SPC     47878.0   \n",
      "9          10      F  32.0            UNM          SPC     26268.0   \n",
      "10         11      M  26.0            MAR          SPC     16793.0   \n",
      "12         13      M  37.0            MAR          SPC     42289.0   \n",
      "13         14      M  43.0            MAR          INP     57567.0   \n",
      "\n",
      "    credit_month tariff_id  score_shk education        living_region  \\\n",
      "7             10       1.1   0.512525       GRD       МОСКОВСКАЯ ОБЛ   \n",
      "9             10       1.1   0.465026       GRD  КРАЙ СТАВРОПОЛЬСКИЙ   \n",
      "10            14       1.0   0.445430       SCH      САНКТ-ПЕТЕРБУРГ   \n",
      "12            10       1.6   0.691609       SCH    ОБЛ АРХАНГЕЛЬСКАЯ   \n",
      "13            10       1.1   0.341164       GRD  ХАНТЫ-МАНСИЙСКИЙ АО   \n",
      "\n",
      "    monthly_income  credit_count  overdue_credit_count  open_account_flg  \n",
      "7          60000.0           3.0                   0.0                 0  \n",
      "9          39500.0           7.0                   0.0                 0  \n",
      "10         36000.0           2.0                   0.0                 0  \n",
      "12         70000.0           1.0                   0.0                 0  \n",
      "13         60000.0           7.0                   0.0                 0  \n",
      "Размер таблицы (161331, 15)\n"
     ]
    }
   ],
   "source": [
    "data_table.dropna(inplace=True) # inplace means we change data_table itself, not create new modified one\n",
    "print(data_table.head())\n",
    "print(f'Размер таблицы {data_table.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь следует обработать колонку living region. \n",
    "Поскольку регионов слишком много, работа сразу со всеми даст громоздкие результаты, анализ которых затруднен.\n",
    "Поэтому мы объединим все регионы по федеральным округам плюс отдельно укажем Москву и Петербург. \n",
    "Получим ровно десять living regions:\n",
    "1. Центральный ФО\n",
    "2. Северо-Западный ФО\n",
    "3. Южный ФО\n",
    "4. Северо-Кавказский ФО\n",
    "5. Приволжский  ФО\n",
    "6. Уральский ФО\n",
    "7. Сибирский ФО\n",
    "8. Дальневосточный ФО\n",
    "9. Москва\n",
    "10. Санкт-Петербург\n",
    "\n",
    "С таким количеством регионов уже можно работать.\n",
    "Попутно отметим, что в таблице два раза встречается регион \"Россия\". Его убираем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the table shape is: (161329, 15)\n",
      "    client_id gender   age marital_status job_position  credit_sum  \\\n",
      "7           8      F  26.0            UNM          SPC     47878.0   \n",
      "9          10      F  32.0            UNM          SPC     26268.0   \n",
      "10         11      M  26.0            MAR          SPC     16793.0   \n",
      "12         13      M  37.0            MAR          SPC     42289.0   \n",
      "13         14      M  43.0            MAR          INP     57567.0   \n",
      "\n",
      "    credit_month tariff_id  score_shk education         living_region  \\\n",
      "7             10       1.1   0.512525       GRD        Центральный ФО   \n",
      "9             10       1.1   0.465026       GRD  Северо-Кавказский ФО   \n",
      "10            14       1.0   0.445430       SCH       Санкт-Петербург   \n",
      "12            10       1.6   0.691609       SCH    Северо-Западный ФО   \n",
      "13            10       1.1   0.341164       GRD          Уральский ФО   \n",
      "\n",
      "    monthly_income  credit_count  overdue_credit_count  open_account_flg  \n",
      "7          60000.0           3.0                   0.0                 0  \n",
      "9          39500.0           7.0                   0.0                 0  \n",
      "10         36000.0           2.0                   0.0                 0  \n",
      "12         70000.0           1.0                   0.0                 0  \n",
      "13         60000.0           7.0                   0.0                 0  \n"
     ]
    }
   ],
   "source": [
    "# looking for the region 'Россия' and removing it:\n",
    "indices = data_table.index[data_table['living_region'] == 'РОССИЯ']\n",
    "for ind in indices:\n",
    "    data_table = data_table.drop(index = ind)\n",
    "\n",
    "# function returning the string replace_to if a data_cell contains a string from string_set\n",
    "def replace_cell_value(data_cell, string_set, replace_to):\n",
    "    for str in string_set:\n",
    "        if str in data_cell:\n",
    "            return replace_to            \n",
    "    return data_cell # if no string in string_set is present in data_cell\n",
    "\n",
    "# the regions of every federal district, keywords:\n",
    "central_fo = ['БЕЛГОРОД', 'БРЯНСК', 'ВЛАДИМИР', 'ВОРОНЕЖ', 'ГУСЬ', 'ИВАНОВ', 'КАЛУГ', 'КАЛУЖ', 'КОСТРОМ', 'КУРСК', 'ЛИПЕЦК', 'МОСКОВСК', 'МЫТИЩ', 'ОРЕЛ', 'ОРЁЛ', 'ОРЛОВСК', 'РЯЗАН', 'СМОЛЕНСК', 'ТАМБОВ', 'ТВЕР', 'ТУЛА', 'ТУЛЬСК', 'ЯРОСЛАВ'] \n",
    "north_west_fo = ['КАРЕЛ','КОМИ','АРХАНГ','ВОЛОГ','КАЛИНИНГР','ЛЕНИНГР','МУРМАН','НОВГОРОД','ПСКОВ','НЕНЕЦК']\n",
    "south_fo = ['АДЫГ','КАЛМЫК','КРЫМ','КРАСНОДАР','АСТРАХАН','ВОЛГОГРАД','РОСТОВ','СЕВАСТ']\n",
    "caucas_fo =['ДАГЕСТ','ИНГУШ','КАБАРДИН','БАЛКАР','КАРАЧАЕВ','ЧЕРКЕС','ОСЕТИ','АЛАНИ','ЧЕЧЕН','ЧЕЧНЯ','СТАВРОПОЛ']\n",
    "volga_fo = ['ПРИВОЛЖСК','БАШКИР','БАШКОР', 'ГОРЬКИЙ', 'ГОРЬКОВСК', 'МАРИЙ','МОРДОВ','ТАТАР','УДМУРТ','ЧУВАШ','ПЕРМ','КИРОВ','НИЖЕГОРОД','НИЖНИЙ','ОРЕНБУРГ','ПЕНЗ','САМАР','САРАТОВ','УЛЬЯН','СИМБИРС']\n",
    "urals_fo =['ЕКАТЕРИНБ','СВЕРДЛОВ','КУРГАН','ТЮМЕН','ЧЕЛЯБ','ТАГИЛ','ХАНТЫ','МАНСИ','ЯМАЛ','НЕНЕЦ', '74']\n",
    "siberia_fo = ['АЛТАЙ','ТЫВА','ТУВА','ТУВИН','ХАКАС','КРАСНОЯРСК','ИРКУТСК','КЕМЕРОВ','КУЗБАСС','НОВОСИБ','ОМСК', 'ЭВЕНК']\n",
    "far_east_fo = ['ВЛАДИВОСТ','ХАБАРОВ','БУРЯТ','САХА','ЯКУТ','БАЙКАЛ','КАМЧАТ','ПРИМОРСК','АМУР','МАГАДАН','САХАЛИН','ЕВРЕЙ','ЧИТА', 'ЧИТИН', 'ЧУКОТ']\n",
    "moskau = ['МОСКВ']\n",
    "piter = ['ПЕТЕРБ', '98']\n",
    "regions = [central_fo, north_west_fo, south_fo, caucas_fo, volga_fo, urals_fo, siberia_fo, far_east_fo, moskau, piter]\n",
    "\n",
    "# regions to be put into dataset:\n",
    "federal_districts = ['Центральный ФО', 'Северо-Западный ФО', 'Южный ФО', 'Северо-Кавказский ФО', 'Приволжский  ФО', 'Уральский ФО', 'Сибирский ФО', 'Дальневосточный ФО', 'Москва', 'Санкт-Петербург']\n",
    "\n",
    "# Replacing the cells in living_region with federal districts\n",
    "i = 0\n",
    "for region in regions:\n",
    "    data_table['living_region'] = data_table['living_region'].map(lambda x: replace_cell_value(x, region, federal_districts[i]))\n",
    "    i += 1\n",
    "\n",
    "# saving result for debugging purposes\n",
    "# data_table.to_csv('tmp.csv', index = False, sep=';', decimal = ',', encoding = 'windows-1251') \n",
    "print(f'Now the table shape is: {data_table.shape}')\n",
    "print(data_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все колонки делим на две категории - числовые и нечисловые. \n",
    "Числовые будут далее исследоваться на выбросы и затем проверяться на нормальность, затем нормироваться.\n",
    "Нечисловые колонки кодируем энкодером OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length of table: 161329\n",
      "New column number: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\varaksa_yua\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\varaksa_yua\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Column titles of categorical features\n",
    "categorical_features_onehot = ['gender', 'marital_status', 'education', 'living_region'] # for encoding by onehot encoder\n",
    "# Even though tariff_id consists from zeros and units, but is in fact non-numerical, since it is the tariff category\n",
    "categorical_features_label = ['tariff_id', 'job_position']  # for encoding by label encoder\n",
    "# Column titles of numerical features\n",
    "numerical_features = ['age', 'credit_sum', 'credit_month', 'score_shk', 'monthly_income', 'credit_count', 'overdue_credit_count', 'open_account_flg']\n",
    "\n",
    "# creating dataframe for encoded values. First including only numeric data there\n",
    "data_table_encoded = data_table[numerical_features] # dataframe for encoded data set\n",
    "data_table_encoded.reset_index(drop=True, inplace = True) # changing series indices to sequential incrementing values\n",
    "# print(data_table_encoded.head()) \n",
    "\n",
    "# transform the categorical data using OneHotEncoder\n",
    "encoder = OneHotEncoder() # init encoder\n",
    "for feature in categorical_features_onehot:\n",
    "    # print(feature)\n",
    "    tmp_data = encoder.fit_transform(data_table[[feature]]) # coded data is sparse matrix\n",
    "    tmp_data = tmp_data.todense() # to usual matrix \n",
    "    # print(tmp_data)\n",
    "    # extract the feature names for the encoded columns\n",
    "    tmp_titles = encoder.get_feature_names_out([feature])\n",
    "    # print(tmp_titles)\n",
    "    tmp_df = pd.DataFrame(tmp_data, columns = tmp_titles)    \n",
    "    data_table_encoded = pd.concat([data_table_encoded, tmp_df], axis = 1, ignore_index = False, sort = False) # merging into the encoded dataframe     \n",
    "    # print(f'Current table size: {data_table_encoded.shape}')\n",
    "\n",
    "# transform other categorical data using LabelEncoder\n",
    "encoder = LabelEncoder() # init encoder\n",
    "for feature in categorical_features_label:\n",
    "    # print(feature)    \n",
    "    data_table_encoded[feature] = encoder.fit_transform(data_table[[feature]])    \n",
    "\n",
    "print(f'New length of table: {len(data_table_encoded)}')\n",
    "print(f'New column number: {data_table_encoded.shape[1]}')\n",
    "\n",
    "# for debugging only:\n",
    "# print(data_table_encoded.head())\n",
    "# data_table_encoded.to_csv('tmp1.csv', sep = ';', decimal = '.', encoding = 'windows-1251')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитываем выбросы и отсеиваем строки с выбросами (где значения отстоят больше чем на 3 стандартных отклонения от среднего).\n",
    "Отметим, что из колонок типа 'overdue_credit_count', 'open_account_flg' отсеивать выбросы не следует. Дело в том, что, например, в колонке 'overdue_credit_count' большинство значений нулевые, и очень редко, например, число просроченных кредитов равно 1, 2 или 3. Тогда среднее значение будет околонулевым, дисперсия из-за подавляющего превосходства в числе нулевых значений тоже будет мала, и поэтому отсеивание выбросов по принципу, применимому для гауссова распределения, то есть отстоящих более чем на три сигмы от среднего значения, просто уберет из колонки все ненулевые значения и тем самым обнулит ценность самой колонки.\n",
    "Поэтому для отсеивания выбросов отберем только колонки, где имеется множество различных значений, а именно: age, credit_sum, score_shk, monthly_income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: age, mean: 37.0, std: 10.4\n",
      "Current table size: 5151456\n",
      "Feature: credit_sum, mean: 25942.8, std: 16194.6\n",
      "Current table size: 5093632\n",
      "Feature: score_shk, mean: 0.5, std: 0.1\n",
      "Current table size: 5089888\n",
      "Feature: monthly_income, mean: 40077.5, std: 24666.0\n",
      "Current table size: 5021984\n",
      "Now the table shape is: (156937, 32)\n"
     ]
    }
   ],
   "source": [
    "# calculating the means and standard deviations over some numerical columns\n",
    "where_remove_outliers = ['age', 'credit_sum', 'score_shk', 'monthly_income']\n",
    "# removing outliers (i.e. leaving only rows withot outliers)\n",
    "# i = 1\n",
    "for feature in where_remove_outliers:\n",
    "    mean = data_table_encoded[feature].mean()\n",
    "    std = data_table_encoded[feature].std()\n",
    "    print(f'Feature: {feature}, mean: {mean:.1f}, std: {std:.1f}')\n",
    "    # z_scores i.e. distance from the data value to mean, in std values\n",
    "    z_scores = (data_table_encoded[feature] - mean) / std # absolute value of z score\n",
    "    z_scores = z_scores.abs()\n",
    "    # print(z_scores)\n",
    "    # z_scores.to_csv('tmp_z'+ str(i) + '.csv', sep = ';', decimal = '.', encoding = 'windows-1251')\n",
    "    # i += 1\n",
    "    # if z_scores are > 3 the data is considered to be an outlier:\n",
    "    data_table_encoded = data_table_encoded[z_scores <= 3] # leaving only items closer to the mean value (not farther than 3*sigma)\n",
    "    # another variant:\n",
    "    # mask = z_scores.abs().lt(3) # mask to remove all rows with z values >=3\n",
    "    # data_table_encoded = data_table_encoded[mask] # leaving only items closer to the mean value (not farther than 3*sigma)\n",
    "    print(f'Current table size: {data_table_encoded.size}')\n",
    " \n",
    "data_table_encoded.reset_index(drop = True, inplace = True) # changing final series indices to incrementing values\n",
    "data_table_encoded.to_csv('tmp_encoded.csv', sep = ';', decimal = '.', encoding = 'windows-1251')   \n",
    "print(f'Now the table shape is: {data_table_encoded.shape}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее следует масштабировать данные - то есть поделить числовые колонки на их максимумы.\n",
    "В финале сохранить полученные предобработанные данные для дальнейшего использования в какой-либо модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156937, 32)\n"
     ]
    }
   ],
   "source": [
    "for feature in numerical_features:\n",
    "    data_table_encoded[feature] = data_table_encoded[feature] / data_table_encoded[feature].max()\n",
    "\n",
    "data_table_encoded.to_csv('credit_train_preprocessed.csv', sep = ';', decimal = '.', encoding = 'windows-1251')  \n",
    "print(data_table_encoded.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учтем, что решаемая нами задача - определение по признакам кредитополучателей, получит ли кредит кредитополучатель, является задачей бинарной классификации. То есть мы должны по набору признаков кредитополучателя предсказать значение в колонке open_account_flg, показывающей, получил ли кредитополучатель кредит.\n",
    "В задачах бинарной классификации бывает целесообразно сбалансировать обучающую выборку, то есть выравнять количество элементов обучающего набора, относящихся к каждому из двух классов кредитополучателей (получивших кредит, т.е. для которых open_account_flg == 1, и не получивших кредит, т.е. для которых open_account_flg == 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_account_flg\n",
      "0.0    131084\n",
      "1.0     25853\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tmp = data_table_encoded['open_account_flg'].value_counts() # extracting the number of each value in the column 'open_account_flg'\n",
    "print(tmp)\n",
    "# extracting all samples with 'open_account_flg'== 1:\n",
    "samples_1 = data_table_encoded[data_table_encoded['open_account_flg'] == 1]\n",
    "# extracting all samples with 'open_account_flg'== 0:\n",
    "samples_0 = data_table_encoded[data_table_encoded['open_account_flg'] == 0]\n",
    "# sampling a sub-sample from samples_0 with the size equal to the one of the  \n",
    "samples_00 = samples_0.sample(n = tmp[1], random_state = 42) # 42 is the seed for random number generator\n",
    "\n",
    "data_table_preprocessed = pd.concat([samples_1, samples_00]) # new data table with equal number of open_account_flg = 0 and = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем размеры нового датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51706, 32)\n",
      "(25853, 32)\n",
      "(25853, 32)\n"
     ]
    }
   ],
   "source": [
    "print(data_table_preprocessed.shape)\n",
    "print(data_table_preprocessed[data_table_preprocessed['open_account_flg'] == 0].shape)\n",
    "print(data_table_preprocessed[data_table_preprocessed['open_account_flg'] == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно разделить датасет на две выборки - обучающую и тестовую.\n",
    "Но вначале нужно разделить таблицу данных на две: на набор колонок X, значения которых будут служить аргументом, подаваемым на вход модели, и колонку меток Y, на получение которой и натренировывается модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data_table_preprocessed.drop(columns=['open_account_flg'])\n",
    "Y = data_table_preprocessed['open_account_flg']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging:\n",
    "# X_train.to_csv('X_train.csv', sep = ';', decimal = '.', encoding = 'windows-1251')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем обучение классификатора GaussianNB (Gaussian Naive Bayesian). При этом в данном случае GridSearchCV для поиска параметров не используется, потому что для GaussianNB никакие параметры не подбираются. Как требуется по заданию, используем make_pipeline.\n",
    "Для любого классификатора в sklearn используется похожая последовательность действий:\n",
    "Создается экземпляр классификатора.\n",
    "Проводится обучение методом fit.\n",
    "Предсказываются значения меток с помощью обученного классификатора вызовом метода predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# creating pipeline\n",
    "pipe = make_pipeline(StandardScaler(), GaussianNB())\n",
    "\n",
    "# training pipeline\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "# predicting class labels\n",
    "Y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes metrics:\n",
      "F1 metrics: 0.46\n",
      "Accuracy metrics: 0.56\n",
      "Area under curve metrics: 0.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# calculating metrics for Gaussian Naive Bayes:\n",
    "F1 = f1_score(Y_test, Y_pred)\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "AUC = roc_auc_score(Y_test, Y_pred)\n",
    "\n",
    "print('Gaussian Naive Bayes metrics:')\n",
    "print(f'F1 metrics: {F1:.2f}')\n",
    "print(f'Accuracy metrics: {accuracy:.2f}')\n",
    "print(f'Area under curve metrics: {AUC:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "Проведена предобработка файла данных по кредитам.\n",
    "Произведена балансировка выборки - то есть отбрасывание части сэмплов с open_account_flg == 0, чтобы выравнять количество кредитополучателей, получивших и не получивших кредит.\n",
    "Произведено обучение моделей с помощью различных методов и выбраны методы, дающие наилучшие результаты согласно используемым метрикам.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
