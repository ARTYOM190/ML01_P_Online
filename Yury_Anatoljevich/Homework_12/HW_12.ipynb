{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ДЗ по итогам лекции 12.\n",
    "\n",
    "ДЗ - в рамках ранее выполненной работы по обработке данных c помощью pandas (учебный файл с данными credit_train.csv), выполнить все работы с пандас дата фреймом через SQL запросы. \n",
    "- разбить выборку на обучающую и тестовую\n",
    "- проанализировать пропуски и решить, что с ними делать\n",
    "- проанализировать выбросы\n",
    "- создать/ удалить переменные\n",
    "- закодировать категориальные переменные\n",
    "- нормализовать числовые переменные (при необходимости)\n",
    "\n",
    "\n",
    "Сама эта ранее выполненная работа включала следующие задания\n",
    "credit_train.csv - файл данных\n",
    "Подготовить данные для модели:\n",
    "    \n",
    "Анализ данных, необходимые корректировки. \n",
    "1. Обработать пропуски. \n",
    "2. Оценить выбросы. \n",
    "3. Корреляция. \n",
    "4. Тест на нормальность распределения.\n",
    "5. Масштабировать данные.\n",
    "\n",
    "В прошлый раз проделывалось следующее: после загрузки файла данных, представленного в формате csv, в датафрейм модуля pandas.\n",
    "*) Далее следует провести поиск пропусков и удалить строки с пропусками.\n",
    "*) Кроме того, предварительный анализ файла данных показывает, что колонка living region содержит много дублей типа \"московская обл\" и \"обл московская\" или \"чеченская республика\" и \"чеченская респ\". Придется поработать с данными этой колонки, чтобы привести систему обозначений регионов к однозначной. Поскольку регионов слишком много, что затруднит анализ корреляции данных, то мы приведем всю систему к федеральным округам и двум столицам - Москва и Санкт-Петербург.\n",
    "*) Далее для оценки выбросов числовых данных следует провести оценку матожидания и дисперсии каждого столбца данных, представленного в численном виде. Все значения, отстоящие более чем на 3 выборочных стандартных отклонения, удаляем из массива данных (точнее, удаляем из выборки все объекты, для которых хотя бы в одном столбце имеются выбросы).\n",
    "*) С помощью функций подключенной библиотек оценим нормальность распределений данных\n",
    "*) В конце числовые колонки данных масштабируем, поделив на максимальный элемент каждой колонки\n",
    "\n",
    "Нынче нужно проделать это же задание через SQL запросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные.\n",
    "Выводим для ознакомления заголовочную строку и первые пять строк. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in the initial table: 170746\n",
      "Initial column number: 15\n",
      "   client_id gender   age marital_status job_position  credit_sum  \\\n",
      "0          1      M   NaN            NaN          UMN    59998.00   \n",
      "1          2      F   NaN            MAR          UMN    10889.00   \n",
      "2          3      M  32.0            MAR          SPC    10728.00   \n",
      "3          4      F  27.0            NaN          SPC    12009.09   \n",
      "4          5      M  45.0            NaN          SPC         NaN   \n",
      "\n",
      "   credit_month tariff_id  score_shk education        living_region  \\\n",
      "0            10       1.6        NaN       GRD   КРАСНОДАРСКИЙ КРАЙ   \n",
      "1             6       1.1        NaN       NaN               МОСКВА   \n",
      "2            12       1.1        NaN       NaN      ОБЛ САРАТОВСКАЯ   \n",
      "3            12       1.1        NaN       NaN    ОБЛ ВОЛГОГРАДСКАЯ   \n",
      "4            10       1.1   0.421385       SCH  ЧЕЛЯБИНСКАЯ ОБЛАСТЬ   \n",
      "\n",
      "   monthly_income  credit_count  overdue_credit_count  open_account_flg  \n",
      "0         30000.0           1.0                   1.0                 0  \n",
      "1             NaN           2.0                   0.0                 0  \n",
      "2             NaN           5.0                   0.0                 0  \n",
      "3             NaN           2.0                   0.0                 0  \n",
      "4             NaN           1.0                   0.0                 0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sqlite3\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "\n",
    "# reading data to pandas dataframe\n",
    "credit_data_table = pd.read_csv('credit_train.csv', sep = ';', decimal = ',', encoding = 'windows-1251')\n",
    "print(f'Rows in the initial table: {len(credit_data_table)}') \n",
    "print(f'Initial column number: {credit_data_table.shape[1]}')\n",
    "print(credit_data_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделяем загруженный датафрейм на два - тренировочный и тестовый. \n",
    "Создаем новую базу данных под оба набора.\n",
    "Загружаем тренировочный и тестовый датафрейм в отдельные таблицы новосозданной базы данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25612"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# splitting dataframe into two sub-dataframes - the training and the test one\n",
    "credit_train, credit_test = train_test_split(credit_data_table, test_size = 0.15, random_state = 0)\n",
    "\n",
    "# creating database SQLite, saving two separate data tables into the same new database:\n",
    "conn = sqlite3.connect('credit_train.db')  # creating new database with the same name as the csv table\n",
    "credit_train.set_index('client_id', inplace=True) # making client id the index of the dataframe\n",
    "credit_test.set_index('client_id', inplace=True) # making client id the index of the dataframe\n",
    "credit_train.to_sql('train_table', conn, if_exists='replace', index=False)  # putting training data to new database\n",
    "credit_test.to_sql('test_table', conn, if_exists='replace', index=False)    # putting test data to new database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обеих таблицах найдем такие записи, которые имеют пропуски. Удалим эти записи из базы данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = credit_train.columns\n",
    "\n",
    "conditions = \" AND \".join([f\"{col} IS NOT NULL\" for col in columns])\n",
    "\n",
    "query = f\"DELETE FROM train_table WHERE NOT ({conditions})\"\n",
    "conn.execute(query)\n",
    "conn.commit()\n",
    "\n",
    "query = f\"DELETE FROM test_table WHERE NOT ({conditions})\"\n",
    "conn.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь следует обработать колонку living region. \n",
    "Поскольку регионов слишком много, работа сразу со всеми даст громоздкие результаты, анализ которых затруднен.\n",
    "Даже Путину это было ясно еще в 2000 г., отчего вся территория России была им поделена на федеральные округа. \n",
    "На данный момент их восемь. Еще имеет смысл отдельно рассматривать Москву и Петербург, поскольку в социологическом плане они сильно отличаются от остальной России.\n",
    "Поэтому мы объединим все регионы по федеральным округам плюс отдельно укажем Москву и Петербург. \n",
    "Получим ровно десять living regions:\n",
    "1. Центральный ФО\n",
    "2. Северо-Западный ФО\n",
    "3. Южный ФО\n",
    "4. Северо-Кавказский ФО\n",
    "5. Приволжский  ФО\n",
    "6. Уральский ФО\n",
    "7. Сибирский ФО\n",
    "8. Дальневосточный ФО\n",
    "9. Москва\n",
    "10. Санкт-Петербург\n",
    "\n",
    "С таким количеством регионов уже можно работать.\n",
    "Попутно отметим, что в таблице два раза встречается регион Россия. Придется его целенаправленно уничтожить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В связи с этим определим функцию, которая будет менять в таблице базы данных одни найденные значения на другие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_data(db_table, column_name, old_value, new_value):\n",
    "    \"\"\" Function replaces in the table db_table, in the column column_name, the value old_value to value new_value\"\"\"\n",
    "    query = f\"UPDATE {db_table} SET {column_name} = ? WHERE {column_name} = ?\"\n",
    "    conn.execute(query, (new_value, old_value))\n",
    "    conn.commit()  # saving result to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем строки, где living_region = 'Россия':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"DELETE FROM train_table WHERE living_region == 'Россия'\"\n",
    "conn.execute(query)\n",
    "conn.commit()\n",
    "\n",
    "query = \"DELETE FROM test_table WHERE living_region == 'Россия'\"\n",
    "conn.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменяем города и веси, расположенные в соответствующем регионе, на названия их федеральных округов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returning the string replace_to if a data_cell contains a string from string_set\n",
    "def replace_cell_value(data_cell, string_set, replace_to):\n",
    "    for str in string_set:\n",
    "        if str in data_cell:\n",
    "            return replace_to            \n",
    "    return data_cell # if no string in string_set is present in data_cell\n",
    "\n",
    "# the regions of every federal district, keywords:\n",
    "central_fo = ['БЕЛГОРОД', 'БРЯНСК', 'ВЛАДИМИР', 'ВОРОНЕЖ', 'ГУСЬ', 'ИВАНОВ', 'КАЛУГ', 'КАЛУЖ', 'КОСТРОМ', 'КУРСК', 'ЛИПЕЦК', 'МОСКОВСК', 'МЫТИЩ', 'ОРЕЛ', 'ОРЁЛ', 'ОРЛОВСК', 'РЯЗАН', 'СМОЛЕНСК', 'ТАМБОВ', 'ТВЕР', 'ТУЛА', 'ТУЛЬСК', 'ЯРОСЛАВ'] \n",
    "north_west_fo = ['КАРЕЛ','КОМИ','АРХАНГ','ВОЛОГ','КАЛИНИНГР','ЛЕНИНГР','МУРМАН','НОВГОРОД','ПСКОВ','НЕНЕЦК']\n",
    "south_fo = ['АДЫГ','КАЛМЫК','КРЫМ','КРАСНОДАР','АСТРАХАН','ВОЛГОГРАД','РОСТОВ','СЕВАСТ']\n",
    "caucas_fo =['ДАГЕСТ','ИНГУШ','КАБАРДИН','БАЛКАР','КАРАЧАЕВ','ЧЕРКЕС','ОСЕТИ','АЛАНИ','ЧЕЧЕН','ЧЕЧНЯ','СТАВРОПОЛ']\n",
    "volga_fo = ['ПРИВОЛЖСК','БАШКИР','БАШКОР', 'ГОРЬКИЙ', 'ГОРЬКОВСК', 'МАРИЙ','МОРДОВ','ТАТАР','УДМУРТ','ЧУВАШ','ПЕРМ','КИРОВ','НИЖЕГОРОД','НИЖНИЙ','ОРЕНБУРГ','ПЕНЗ','САМАР','САРАТОВ','УЛЬЯН','СИМБИРС']\n",
    "urals_fo =['ЕКАТЕРИНБ','СВЕРДЛОВ','КУРГАН','ТЮМЕН','ЧЕЛЯБ','ТАГИЛ','ХАНТЫ','МАНСИ','ЯМАЛ','НЕНЕЦ', '74']\n",
    "siberia_fo = ['АЛТАЙ','ТЫВА','ТУВА','ТУВИН','ХАКАС','КРАСНОЯРСК','ИРКУТСК','КЕМЕРОВ','КУЗБАСС','НОВОСИБ','ОМСК', 'ЭВЕНК']\n",
    "far_east_fo = ['ВЛАДИВОСТ','ХАБАРОВ','БУРЯТ','САХА','ЯКУТ','БАЙКАЛ','КАМЧАТ','ПРИМОРСК','АМУР','МАГАДАН','САХАЛИН','ЕВРЕЙ','ЧИТА', 'ЧИТИН', 'ЧУКОТ']\n",
    "moskau = ['МОСКВ']\n",
    "piter = ['ПЕТЕРБ', '98']\n",
    "regions = [central_fo, north_west_fo, south_fo, caucas_fo, volga_fo, urals_fo, siberia_fo, far_east_fo, moskau, piter]\n",
    "\n",
    "# regions to be put into dataset:\n",
    "federal_districts = ['Центральный ФО', 'Северо-Западный ФО', 'Южный ФО', 'Северо-Кавказский ФО', 'Приволжский  ФО', 'Уральский ФО', 'Сибирский ФО', 'Дальневосточный ФО', 'Москва', 'Санкт-Петербург']\n",
    "\n",
    "# Replacing the cells in living_region with federal districts\n",
    "i = 0\n",
    "for region in regions:\n",
    "    data_table['living_region'] = data_table['living_region'].map(lambda x: replace_cell_value(x, region, federal_districts[i]))\n",
    "    i += 1\n",
    "\n",
    "# saving result for debugging purposes\n",
    "# data_table.to_csv('tmp.csv', index = False, sep=';', decimal = ',', encoding = 'windows-1251') \n",
    "print(f'Now the table shape is: {data_table.shape}')\n",
    "print(data_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все колонки делим на две категории - числовые и нечисловые. \n",
    "Числовые будут далее исследоваться на выбросы и затем проверяться на нормальность, затем нормироваться.\n",
    "Нечисловые колонки кодируем энкодером OneHotEncoder.\n",
    "Особняком стоит колонка open_account_flg - флаг, сигнализирующий, открыт ли счет. Имеет два значения 0 и 1, а значит, по факту это нечисловая колонка \"Счет открыт\" или \"Счет закрыт\".\n",
    "Поэтому с ней возникает вопрос - стоит ли ее кодировать с помощью OneHotEncoder или нет. По виду она уже сейчас не отличается от любой колонки, получаемой на выходе OneHotEncoder, поэтому кодировать ее и превращать тем самым в две колонки не будем.\n",
    "Кроме того, колонка tariff_id перечисляет множество тарифов, относящихся к заемщикам. Это множество тарифов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column titles of categorical features\n",
    "categorical_features_onehot = ['gender', 'marital_status', 'education', 'living_region'] # for encoding by onehot encoder\n",
    "# Even though tariff_id consists from zeros and units, but is in fact non-numerical, since it is the tariff category\n",
    "categorical_features_label = ['tariff_id', 'job_position']  # for encoding by label encoder\n",
    "# Column titles of numerical features\n",
    "numerical_features = ['age', 'credit_sum', 'credit_month', 'score_shk', 'monthly_income', 'credit_count', 'overdue_credit_count', 'open_account_flg']\n",
    "\n",
    "# creating dataframe for encoded values. First including only numeric data there\n",
    "data_table_encoded = data_table[numerical_features] # dataframe for encoded data set\n",
    "data_table_encoded.reset_index(drop=True, inplace = True) # changing series indices to sequential incrementing values\n",
    "# print(data_table_encoded.head()) \n",
    "\n",
    "# transform the categorical data using OneHotEncoder\n",
    "encoder = OneHotEncoder() # init encoder\n",
    "for feature in categorical_features_onehot:\n",
    "    # print(feature)\n",
    "    tmp_data = encoder.fit_transform(data_table[[feature]]) # coded data is sparse matrix\n",
    "    tmp_data = tmp_data.todense() # to usual matrix \n",
    "    # print(tmp_data)\n",
    "    # extract the feature names for the encoded columns\n",
    "    tmp_titles = encoder.get_feature_names_out([feature])\n",
    "    # print(tmp_titles)\n",
    "    tmp_df = pd.DataFrame(tmp_data, columns = tmp_titles)    \n",
    "    data_table_encoded = pd.concat([data_table_encoded, tmp_df], axis = 1, ignore_index = False, sort = False) # merging into the encoded dataframe     \n",
    "    # print(f'Current table size: {data_table_encoded.shape}')\n",
    "\n",
    "# transform other categorical data using LabelEncoder\n",
    "encoder = LabelEncoder() # init encoder\n",
    "for feature in categorical_features_label:\n",
    "    # print(feature)    \n",
    "    data_table_encoded[feature] = encoder.fit_transform(data_table[[feature]])    \n",
    "\n",
    "print(f'New length of table: {len(data_table_encoded)}')\n",
    "print(f'New column number: {data_table_encoded.shape[1]}')\n",
    "\n",
    "# for debugging only:\n",
    "# print(data_table_encoded.head())\n",
    "# data_table_encoded.to_csv('tmp1.csv', sep = ';', decimal = '.', encoding = 'windows-1251')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитываем выбросы и отсеиваем строки с выбросами (где значения отстоят больше чем на 3 стандартных отклонения от среднего).\n",
    "Отметим, что из колонок типа 'overdue_credit_count', 'open_account_flg' отсеивать выбросы не следует. Дело в том, что, например, в колонке 'overdue_credit_count' большинство значений нулевые, и очень редко, например, число просроченных кредитов равно 1, 2 или 3. Тогда среднее значение будет околонулевым, дисперсия из-за подавляющего превосходства в числе нулевых значений тоже будет мала, и поэтому отсеивание выбросов по принципу, применимому для гауссова распределения, то есть отстоящих более чем на три сигмы от среднего значения, просто уберет из колонки все ненулевые значения и тем самым обнулит ценность самой колонки.\n",
    "Поэтому для отсеивания выбросов отберем только колонки, где имеется множество различных значений, а именно: age, credit_sum, credit_month, score_shk, monthly_income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the means and standard deviations over some numerical columns\n",
    "where_remove_outliers = ['age', 'credit_sum', 'credit_month', 'score_shk', 'monthly_income']\n",
    "# removing outliers (i.e. leaving only rows withot outliers)\n",
    "# i = 1\n",
    "for feature in where_remove_outliers:\n",
    "    mean = data_table_encoded[feature].mean()\n",
    "    std = data_table_encoded[feature].std()\n",
    "    print(f'Feature: {feature}, mean: {mean:.1f}, std: {std:.1f}')\n",
    "    # z_scores i.e. distance from the data value to mean, in std values\n",
    "    z_scores = (data_table_encoded[feature] - mean) / std # absolute value of z score\n",
    "    z_scores = z_scores.abs()\n",
    "    # print(z_scores)\n",
    "    # z_scores.to_csv('tmp_z'+ str(i) + '.csv', sep = ';', decimal = '.', encoding = 'windows-1251')\n",
    "    # i += 1\n",
    "    # if z_scores are > 3 the data is considered to be an outlier:\n",
    "    data_table_encoded = data_table_encoded[z_scores <= 3] # leaving only items closer to the mean value (not farther than 3*sigma)\n",
    "    # another variant:\n",
    "    # mask = z_scores.abs().lt(3) # mask to remove all rows with z values >=3\n",
    "    # data_table_encoded = data_table_encoded[mask] # leaving only items closer to the mean value (not farther than 3*sigma)\n",
    "    print(f'Current table size: {data_table_encoded.size}')\n",
    " \n",
    "data_table_encoded.reset_index(drop = True, inplace = True) # changing final series indices to incrementing values\n",
    "# for debugging only:\n",
    "# print(data_table_encoded.head())\n",
    "data_table_encoded.to_csv('tmp_encoded.csv', sep = ';', decimal = '.', encoding = 'windows-1251')   \n",
    "print(f'Now the table shape is: {data_table_encoded.shape}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расчет и визуализация корреляционной матрицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# picking the numeric columns only: \n",
    "# corr = data_table_encoded[numerical_features].corr()\n",
    "# using all the columns:\n",
    "corr = data_table_encoded.corr()\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(corr, fmt='.2f', annot = True, cmap = 'coolwarm', annot_kws={\"fontsize\":8}) # cmap: 'seismic', 'coolwarm', YlGnBu\", \"Blues\", \"coolwarm\", \"BuPu\", \"Greens\", \"Oranges\", \"Reds\", \"Purples\", \"YlOrBr\"\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(\"Correlation matrix\", fontsize = 14)\n",
    "plt.savefig('CorrMatrix.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из корреляционной матрицы намечается:\n",
    "Положительная корреляция между суммой кредита и месячным доходом.\n",
    "Отрицательная корреляция между возрастом и статусом \"не в браке\".\n",
    "Положительная корреляция между возрастом и статусом \"вдовство\".\n",
    "Положительная корреляция между суммой и сроком кредита.\n",
    "Отрицательная корреляция между числом взятых кредитов и кредитным рейтингом гражданина.\n",
    "У месячного дохода есть положительная корреляция с мужским полом и такая же отрицательная корреляция с женским полом.\n",
    "Видна также корреляция между id тарифа и кредитным рейтингом.\n",
    "Сильная корреляция наблюдается в парах априорно взаимоисключающих статусов, например \"в браке\" - \"не в браке\", \"образование школьное\" - \"образование базовое\".\n",
    "Что касается регионов, то намечается положительная корреляция между месячным доходом и проживанием в Москве, и гораздо слабее - с проживанием в Петербурге.\n",
    "Впрочем, в целом коэффициенты корреляции не настолько велики, чтобы можно было на них опираться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест на нормальность распределения.\n",
    "Проверка с помощью теста Шапиро-Вилка в нашем случае не годится, поскольку для N>5000 практически всегда гипотеза нормального распределения отвергается. А у нас около двухсот тысяч строк в файле.\n",
    "То есть в нашем случае почти наверняка результат будет отрицательным.\n",
    "Но проверить не худо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "for feature in numerical_features:\n",
    "    print(feature)\n",
    "    # print(data_table_encoded[feature])\n",
    "    s, p = shapiro(data_table_encoded[feature])\n",
    "    print(f'The p-value is {p}')\n",
    "    if p > 0.05:\n",
    "        print(f'The null hypothesis (that the data came from a normally distributed population)\\n can not be rejected for {feature}')\n",
    "    else:\n",
    "        print(f'There is evidence that the data in {feature} are not normally distributed')     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, значения доверительной вероятности получились весьма малыми, и даже выскочило предупреждение о неприменимости теста.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое простое, что можно сделать - ограничить размер набора данных. Например, посчитать р-value по выборке мощностью 100 единиц.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numerical_features:      \n",
    "    s, p = shapiro(data_table_encoded[feature][1:100])\n",
    "    print(f'The p-value is {p}')\n",
    "    if p > 0.05:\n",
    "        print(f'The null hypothesis (that the data came from a normally distributed population) \\ncan not be rejected for {feature}\\n')\n",
    "    else:\n",
    "        print(f'There is evidence that the data in {feature} are not normally distributed\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что для набора из 100 элементов выборки нельзя отвергнуть гипотезу о нормальном распределении данных для кредитного рейтинга (score_shk) и числа просроченных кредитов (overdue_credit_count)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим выборки на нормальность с помощью текста Андерсона-Дарлинга, который должен работать для больших выборок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson\n",
    "for feature in numerical_features:  \n",
    "    print(f'{feature}:')    \n",
    "    res = anderson(data_table_encoded[feature], dist='norm') # Anderson-Darling test for normality of the sampled set\n",
    "    # print(res)\n",
    "    # print(f'Critical values: {res.critical_values}')\n",
    "    # print(f'Significance_level: {res.critical_values}')\n",
    "    print(f'The p-value for 5% is {res.critical_values[2]}\\n')\n",
    "    \"\"\"if p > 0.05:\n",
    "        print(f'The null hypothesis (that the data came from a normally distributed population) \\ncan not be rejected for {feature}\\n')\n",
    "    else:\n",
    "        print(f'There is evidence that the data in {feature} are not normally distributed\\n')\n",
    "    \"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее следует масштабировать данные - то есть поделить числовые колонки на их максимумы.\n",
    "В финале сохранить полученные предобработанные данные для дальнейшего использования в какой-либо модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numerical_features:\n",
    "    data_table_encoded[feature] = data_table_encoded[feature] / data_table_encoded[feature].max()\n",
    "\n",
    "data_table_encoded.to_csv('credit_train_preprocessed.csv', sep = ';', decimal = '.', encoding = 'windows-1251')  \n",
    "print(data_table_encoded.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "Проведена предобработка файла данных по кредитам.\n",
    "Убраны все строки данных с пропусками.\n",
    "Регионы выдачи кредитов поделены на 10 групп - Москва, Петербург, и 8 федеральных округов.\n",
    "Построена матрица корреляции числовых колонок данных. \n",
    "Указания на наличие корреляции (хотя значение коэффициента корреляции недостаточно высокое, чтобы уверенно утверждать корреляцию данных) получено для следующих параметров:\n",
    "Положительная корреляция между суммой кредита и месячным доходом - логично, банки дают более высокие кредиты гражданам с более высоким доходом.\n",
    "Отрицательная корреляция между возрастом и статусом \"не в браке\" - логично, молодежь еще не повыскакивала замуж/не поженилась.\n",
    "Положительная корреляция между возрастом и статусом \"вдовство\" - логично, все мы смертны.\n",
    "Положительная корреляция между суммой и сроком кредита - логично, чем больше кредит, тем дольше его отдавать.\n",
    "Отрицательная корреляция между числом взятых кредитов и кредитным рейтингом гражданина - тоже логично, кредитный рейтинг опирается на число уже взятых кредитов.\n",
    "У месячного дохода есть положительная корреляция с мужским полом и такая же отрицательная корреляция с женским полом, что логично, учитывая гендерное неравенство.\n",
    "Видна также корреляция между id тарифа и кредитным рейтингом, скорее всего обусловленная тем, что именно кредитный рейтинг служит источником для присвоения того или иного тарифа.\n",
    "Также сильная корреляция наблюдается в парах априорно взаимоисключающих статусов, например \"в браке\" - \"не в браке\", \"образование школьное\" - \"образование базовое\", которая, конечно, не несет полезной информации.\n",
    "Что касается регионов, то намечается положительная корреляция между месячным доходом и проживанием в Москве, и гораздо слабее - с проживанием в Петербурге.\n",
    "\n",
    "Были убраны выбросы и проведена нормировка данных.\n",
    "\n",
    "Мы получили, что обработанный файл стал гораздо больше исходного, несмотря на то, что множество строк с данными было отсеяно из-за неполноты или плохого качества данных (изначально файл содержал 170 тысяч строк, теперь 143 тысячи строк). В первую очередь распухание файла данных обусловлено использованием OneHotEncoder, который для каждого значения категориальных данных создает свою колонку с бинарными значениями, отчего в предобработанном файле оказывается гораздо больше колонок, чем в исходном.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
