{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание:\n",
    "1.\tЗагрузить файл длиной не менее 2000 символов. \n",
    "2.\tСоставить программу, которая считает число уникальных слов в тексте (без критерия схожести)\n",
    "3.\tСоставить программу, которая считает число гласных и согласных букв. \n",
    "4.\tСоставить программу, которая считает число предложений, их длину и число (количество) раз использования каждого слова в тексте (с критерием схожести, критерий схожести слов выбрать самостоятельно, например, spacy (en_core_web_sm) или расстояние Левенштейна). \n",
    "5.\tВывести 10 наиболее часто встречаемых слов. \n",
    "\n",
    "***\n",
    "В данном блокноте выполняем пункт 4 и 5 задания.\n",
    "\n",
    "***\n",
    "Анализ по данным пунктам задания:\n",
    "a) Для подсчета числа предложений можно находить знаки конца предложения (точку, вопросительный и восклицательный знак). Если между предыдущим таким знаком (или началом текста) и текущим знаком находится хотя бы один алфавитно-цифровой знак, то это и будет предложение.\n",
    "Таким образом, можно разбить текст на список предложений. Длина списка будет числом предложений в тексте.\n",
    "b) Для отыскания длины каждого предложения можно предварительно каждое предложение в списке от знаков препинания, заменив их пробелами, и расщепить на список слов с помощью соответствующей процедуры Python. Длина этого списка будет длиной предложения.\n",
    "c) Для нахождения числа раз использования каждого слова в тексте с использованием критерия схожести используем библиотеку spacy. \n",
    "Вначале мы уберем из текста все знаки препинания и разделим текст на отдельные слова. Получим множество всех слов текста. Используя процедуру, отработанную при выполнении второго пункта задания, уберем незначащие слова из списка.\n",
    "Далее для каждого слова из множества, используя функции библиотеки spacy, найдем список похожих на него слов. Далее, имея набор списков похожих слов, для каждого слова каждого списка найдем количество его вхождений в исходный очищенный текст. Взяв сумму числа вхождений для слов каждого списка, получим искомое число раз использования каждого слова в тексте.\n",
    "Чтобы не было путаницы с регистрами текста, предварительно приводим весь текст к нижнему регистру.\n",
    "\n",
    "Тогда план работы по пункту 2 следующий.\n",
    "\n",
    "1. Загрузить текст из файла.\n",
    "2. Перевести текст в нижний регистр.\n",
    "3. Проходить строку, включающую исследуемый текст, знак за знаком, находить знак конца предложения, то есть точку, восклицательный или положительный знак, и его положение в тексте. Для точки проверять, чтобы справа и слева не было цифр, иначе это будет десятичная точка числа, а не знак конца предложения.\n",
    "4. Взяв слайс между предыдущим и текущим знаком конца предложения, проверить, есть ли в этом слайсе алфавитно-цифровые знаки. В случае наличия считаем полученный слайс очередным предложением текста и заносим его в список предложений.\n",
    "5. Далее брать по одному предложению из списка и создавать из него новую строку, включающую только алфавитно-цифровые знаки, остальные знаки заменять пробелами. Затем расщеплять полученную строку процедурой split и находить длину полученного списка. Это будет длина предложения в словах. Выводить результат на экран.\n",
    "6. Убрать в тексте, полученном в пункте 2, знаки препинания и шире - все неалфавитные знаки, заменив их пробелами.\n",
    "7. Разбить текст на слова процедурой split, считая пробелы разделителями.\n",
    "8. Из полученного списка слов убрать все стоп-слова.\n",
    "9. Получить множество уникальных (пока без критерия схожести) слов текста, преобразовав список в множество.\n",
    "10. Загрузить модель spacy en_core_web_sm. Для каждого слова множества с помощью данной модели найти схожие с ним слова. Список похожих слов добавить в словарь в качестве ключа. Эти похожие слова удалить из множества уникальных слов. Повторять для каждого следующего слова, пока слова в множестве не кончатся.\n",
    "11. Для каждого ключа полученного словаря проверить число вхождений в очищенный текст каждого слова ключа. Суммарное число вхождений присвоить как значения поля value данного ключа.\n",
    "12. Отсортировать полученный словарь по значению ключа (дающего частотность соответствующего слова)\n",
    "13. Отобрать 10 наиболее часто встречающихся слов в полученном словаре.\n",
    "14. Если в полученном словаре есть еще слова с той же частотностью, что и у десятого слова, также включить их в выводимый результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загружаем текстовый файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read successfully\n"
     ]
    }
   ],
   "source": [
    "import string, spacy\n",
    "\n",
    "file_name = 'GospelJohn.txt' # file with text\n",
    "file_name = 'example.txt' # file with text\n",
    "try:\n",
    "    f = open(file_name,\"r\") # open file for reading\n",
    "    text = f.read()         # reading file \n",
    "    f.close()               # closing file\n",
    "    print('Read successfully')\n",
    "except:\n",
    "    print('Error reading file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Переводим текст в нижний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lower case:\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Проходить строку, включающую исследуемый текст, знак за знаком, находить знак конца предложения, то есть точку, восклицательный или положительный знак, и его положение в тексте. Для точки проверять, чтобы справа и слева не было цифр, иначе это будет десятичная точка числа, а не знак конца предложения.\n",
    "4. Взяв слайс между предыдущим и текущим знаком конца предложения, проверить, есть ли в этом слайсе алфавитно-цифровые знаки. В случае наличия считаем полученный слайс очередным предложением текста и заносим его в список предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot is reached, i = 3\n",
      "u\n",
      "\n",
      "\n",
      "The dot is reached, i = 17\n",
      "s\n",
      " \n",
      "The dot is reached, i = 35\n",
      "t\n",
      "a\n",
      "String:\n",
      "ou.\n",
      "the thing is. the other is not.are you here\n",
      "String:\n",
      "jackie\n",
      "Lengths of sentences in the text (in words): [10, 1]\n"
     ]
    }
   ],
   "source": [
    "# \"left\" - variable for left-hand end of a sentence\n",
    "right = -1 # variable for right-hand end of a sentence\n",
    "i = -1 # counter\n",
    "sentences_length_list = list() # list for sentences lengths\n",
    "for c in text:\n",
    "    i += 1\n",
    "    if c in \".?!\":\n",
    "        if c == '.' and text[i-1].isnumeric and text[i+1].isnumeric:\n",
    "            print(f'The dot is reached, i = {i}')\n",
    "            print(text[i-1])\n",
    "            print(text[i+1])\n",
    "            continue\n",
    "        # new boundaries of a string:\n",
    "        left = right + 1\n",
    "        right = i\n",
    "        astring = text[left:right] # extract the string\n",
    "        print(f'String:\\n{astring}')\n",
    "        # if the string contains alphanumeric symbols, we suppose for it to be a sentence\n",
    "        if len([ch for ch in astring if ch.isalnum()]) > 0:\n",
    "            sentence_alphanumeric = \"\".join([ch if ch.isalnum else \" \" for ch in astring])\n",
    "            sentence_length = len(sentence_alphanumeric.split()) # length of sentence in words\n",
    "            sentences_length_list.append(sentence_length)\n",
    "print(f'Lengths of sentences in the text (in words): {sentences_length_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = 'The string'\n",
    "str[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', 's', 't', 'r', 'i', 'n', 'g', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "a = \"the string 123!?<>\"\n",
    "lst = [ch for ch in a if ch.isalnum()]\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем не-алфавитные знаки и разбиваем текст на слова, помещаем их в список."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace non-alphabetic signs by whitespaces:\n",
    "text_no_signs = \"\".join([c if c.isalpha() else \" \" for c in text])\n",
    "\n",
    "word_list = text_no_signs.split() # split text into words and put to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем из списка лишние слова (\"стоп-слова\").\n",
    "Для этого вначале создаем список стоп-слов\n",
    "(источник: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)\n",
    "Также мы немного расширяем этот список для учета устаревших слов, фигурирующих в нашем тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(\n",
    "    \"\"\"\n",
    "a about above across after afterwards again against all almost alone along\n",
    "already also although always am among amongst amount an and another any anyhow\n",
    "anyone anything anyway anywhere are around as at\n",
    "\n",
    "back be became because become becomes becoming been before beforehand behind\n",
    "being below beside besides between beyond both bottom but by\n",
    "\n",
    "call can cannot ca could\n",
    "\n",
    "did do does doing done down due during\n",
    "\n",
    "each eight either eleven else elsewhere empty enough even ever every\n",
    "everyone everything everywhere except\n",
    "\n",
    "few fifteen fifty first five for former formerly forty four from front full\n",
    "further\n",
    "\n",
    "get give go\n",
    "\n",
    "had has have he hence her here hereafter hereby herein hereupon hers herself\n",
    "him himself his how however hundred\n",
    "\n",
    "i if in indeed into is it its itself\n",
    "\n",
    "keep\n",
    "\n",
    "last latter latterly least less\n",
    "\n",
    "just\n",
    "\n",
    "made make many may me meanwhile might mine more moreover most mostly move much\n",
    "must my myself\n",
    "\n",
    "name namely neither never nevertheless next nine no nobody none noone nor not\n",
    "nothing now nowhere\n",
    "\n",
    "of off often on once one only onto or other others otherwise our ours ourselves\n",
    "out over own\n",
    "\n",
    "part per perhaps please put\n",
    "\n",
    "quite\n",
    "\n",
    "rather re really regarding\n",
    "\n",
    "same say see seem seemed seeming seems serious several she should show side\n",
    "since six sixty so some somehow someone something sometime sometimes somewhere\n",
    "still such\n",
    "\n",
    "take ten than that the their them themselves then thence there thereafter\n",
    "thereby therefore therein thereupon these they third this those though three\n",
    "through throughout thru thus to together too top toward towards twelve twenty\n",
    "two\n",
    "\n",
    "under until up unless upon us used using\n",
    "\n",
    "various very very via was we well were what whatever when whence whenever where\n",
    "whereafter whereas whereby wherein whereupon wherever whether which while\n",
    "whither who whoever whole whom whose why will with within without would\n",
    "\n",
    "yet you your yours yourself yourselves\n",
    "\"\"\".split()\n",
    ")\n",
    "\n",
    "contractions = [\"n't\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\"]\n",
    "STOP_WORDS.update(contractions)\n",
    "\n",
    "for apostrophe in [\"‘\", \"’\"]:\n",
    "    for stopword in contractions:\n",
    "        STOP_WORDS.add(stopword.replace(\"'\", apostrophe))\n",
    "\n",
    "obsoletisms = [\"art\", \"hath\", \"thou\", \"thee\", \"unto\"] # obsolete word forms\n",
    "STOP_WORDS.update(obsoletisms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем из списка слова, если они входят в список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_new = list()\n",
    "for aword in word_list:\n",
    "    if aword not in STOP_WORDS:\n",
    "        word_list_new.append(aword)\n",
    "\n",
    "# print(len(word_list))\n",
    "# print(len(word_list_new))\n",
    "word_list = word_list_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проходим по всему набору слов и вносим ранее не встреченные слова в словарь, для ранее встреченных увеличиваем value на единицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = dict() # empty dictionary for all words\n",
    "# putting all the words into dict:\n",
    "for aword in word_list:\n",
    "    if aword in word_dict: # if already in dict\n",
    "        word_dict[aword] += 1            \n",
    "    else:                  # if not in dict yet\n",
    "        word_dict[aword] = 1  \n",
    "                 \n",
    "# print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим число уникальных слов (без критерия похожести; \"стоп-слова\" уже удалены и не учитываются)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of the unique words in the text: 208\n"
     ]
    }
   ],
   "source": [
    "key_number = len(word_dict.keys())\n",
    "print(f'The number of the unique words in the text: {key_number}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сортируем словарь по увеличению частоты встречаемости слова, извлекаем десять последних (то есть самых частых) слов.\n",
    "Если 11-е, 12-е и т.д. слова встречаются так же часто, как и десятое, выводим также и их, потому что по факту они все делят десятое место в списке наиболее частых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent words:\n",
      "1. The word \"And\" has the frequency 20\n",
      "2. The word \"I\" has the frequency 16\n",
      "3. The word \"saith\" has the frequency 13\n",
      "4. The word \"said\" has the frequency 12\n",
      "5. The word \"Jesus\" has the frequency 12\n",
      "6. The word \"God\" has the frequency 12\n",
      "7. The word \"John\" has the frequency 9\n",
      "8. The word \"He\" has the frequency 8\n",
      "9. The word \"saw\" has the frequency 7\n",
      "10. The word \"sent\" has the frequency 6\n",
      "9. The word \"man\" has frequency 6\n"
     ]
    }
   ],
   "source": [
    "# sorting the list of the most frequent words:\n",
    "word_dict_sorted = sorted(word_dict.items(), key = lambda item: item[1])\n",
    "print('The most frequent words:')\n",
    "\n",
    "# extracting the most frequent words\n",
    "for i in range(10):\n",
    "    aword = word_dict_sorted[-i-1][0]\n",
    "    frequency = word_dict_sorted[-i-1][1]\n",
    "    print(f'{i+1}. The word \"{aword}\" has the frequency {frequency}')\n",
    "\n",
    "# extracting also the 11th, 12th etc, if the frequency is the same as for the 10th one:\n",
    "while word_dict_sorted[-i-2][1] == word_dict_sorted[-10][1]:\n",
    "    aword = word_dict_sorted[-i-2][0]\n",
    "    frequency = word_dict_sorted[-i-2][1]\n",
    "    print(f'{i}. The word \"{aword}\" has frequency {frequency}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, нами был проделан анализ англоязычного текста в отношении количества уникальных слов без объединения словоформ по критерию схожести.\n",
    "\n",
    "Выводы:\n",
    "1. При анализе числа уникальных слов в тексте следует удалять т. наз. \"стоп-слова\", то есть артикли, предлоги и прочие слова, не несущие смысловой нагрузки.\n",
    "2. Для исключения ситуации, когда слова в начале и середине предложения считаются за разные слова за счет отличия регистра первой буквы, следует привести слова перед анализом к одному регистру.\n",
    "3. В классическом англоязычном тексте 17 в. на ~1000 слов (4102 знака без учета пробелов) после удаления стоп-слов оказалось 170 уникальных слов (без дальнейшего объединения словоформ по критерию схожести), то есть число уникальных значащих слов примерно в пять раз меньше, чем общее число всех слов.\n",
    "4. Десять отобранных уникальных слов с максимальной частотностью дают хорошее представление о тематике текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
