{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание по теме PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для произвольно выбранного датасета провести обработку данных и построить предсказательную модель с использованием функционала PySpark. <br>\n",
    "\n",
    "Мы выбрали датасет Walmart.csv и с помощью pyspark вместо pandas+sklearn решали задачу регрессии, то есть по имеющимся данным строили модель, описывающую пятничные продажи в зависимости от набора параметров.\n",
    "\n",
    "- План работы: <br>\n",
    "Следует загрузить файл csv датасета, используя pyspark.<br>\n",
    "Произвести предобработку данных (выкинуть строки с пропусками). <br>\n",
    "Выделить независимые переменные - характеристики (features) модели.<br>\n",
    "Выделить колонку целевой (target) переменной - суммы пятничных продаж.<br>\n",
    "Разделить данные на тренировочный и тестовый датасет.<br>\n",
    "Создать модель (можно с использованием пайплайна).<br>\n",
    "Подобрать гиперпараметры.<br>\n",
    "Провести проверку модели на тестовой выборке. Рассчитать метрики на тестовой выборке. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка необходимых библиотек\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Костыль для работы в VS Code (вроде и без него работало, но оставили на всякий случай):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск сессии pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Walmart Sales Prediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+------------+-----------+----------+-----------+------------+\n",
      "|Store|      Date|     Sales|Holiday_Flag|Temperature|Fuel_Price|        CPI|Unemployment|\n",
      "+-----+----------+----------+------------+-----------+----------+-----------+------------+\n",
      "|    1|05-02-2010| 1643690.9|           0|      42.31|     2.572|211.0963582|       8.106|\n",
      "|    1|12-02-2010|1641957.44|           1|      38.51|     2.548|211.2421698|       8.106|\n",
      "|    1|19-02-2010|1611968.17|           0|      39.93|     2.514|211.2891429|       8.106|\n",
      "|    1|26-02-2010|1409727.59|           0|      46.63|     2.561|211.3196429|       8.106|\n",
      "|    1|05-03-2010|1554806.68|           0|       46.5|     2.625|211.3501429|       8.106|\n",
      "|    1|12-03-2010|1439541.59|           0|      57.79|     2.667|211.3806429|       8.106|\n",
      "|    1|19-03-2010|1472515.79|           0|      54.58|      2.72| 211.215635|       8.106|\n",
      "|    1|26-03-2010|1404429.92|           0|      51.45|     2.732|211.0180424|       8.106|\n",
      "|    1|02-04-2010|1594968.28|           0|      62.27|     2.719|210.8204499|       7.808|\n",
      "|    1|09-04-2010|1545418.53|           0|      65.86|      2.77|210.6228574|       7.808|\n",
      "|    1|16-04-2010|1466058.28|           0|      66.32|     2.808|   210.4887|       7.808|\n",
      "|    1|23-04-2010|1391256.12|           0|      64.84|     2.795|210.4391228|       7.808|\n",
      "|    1|30-04-2010|1425100.71|           0|      67.41|      2.78|210.3895456|       7.808|\n",
      "|    1|07-05-2010|1603955.12|           0|      72.55|     2.835|210.3399684|       7.808|\n",
      "|    1|14-05-2010| 1494251.5|           0|      74.78|     2.854|210.3374261|       7.808|\n",
      "+-----+----------+----------+------------+-----------+----------+-----------+------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "Data types:\n",
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Holiday_Flag: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- CPI: double (nullable = true)\n",
      " |-- Unemployment: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "data = spark.read.csv('Walmart.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Просмотр первых 15 строк\n",
    "data.show(15)\n",
    "\n",
    "# Проверка типов данных\n",
    "print(\"Data types:\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Store: int, Date: string, Sales: double, Holiday_Flag: int, Temperature: double, Fuel_Price: double, CPI: double, Unemployment: double]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выкидываем пропуски на тот случай, если они есть. Реально их нету :)\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди данных есть колонка с датами. Для построения модели регрессии дата не нужна.\n",
    "Остальные параметры - номер магазина, совпадение с праздником, температура, стоимость топлива, индекс потребительских цен, уровень безработицы, - являются потенциально важными для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление колонки с датами\n",
    "data = data.drop('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предполагаем, что 'Sales' - это целевая переменная, а остальные - признаки\n",
    "feature_columns = [column for column in data.columns if (column != 'Sales')]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Разделение данных на тренировочную и тестовую выборки\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42) # в качестве затравочного случайного значения, естественно, используем 42 :)\n",
    "\n",
    "# Определение модели и пайплайна\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='Sales')\n",
    "pipeline = Pipeline(stages=[rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск оптимальных значений гиперпараметров "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка сетки гиперпараметров\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [100, 150]) \\\n",
    "    .addGrid(rf.maxDepth, [15, 20]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [2, 5]) \\\n",
    "    .build()\n",
    "\n",
    "# Кросс-валидация\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol='Sales', metricName='rmse'),\n",
    "                          parallelism=6, # since we have 6 cores in CPU here\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели и выбор наилучшего набора параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "cv_model = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем посмотреть набор наилучших параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "Num Trees: 150\n",
      "Max Depth: 20\n",
      "Min Instances per Node: 2\n"
     ]
    }
   ],
   "source": [
    "# Получение наилучшей модели из CrossValidator\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Получение параметров модели\n",
    "rf_model = best_model.stages[0]  # RandomForestRegressor находится на первом месте в пайплайне\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(f\"Num Trees: {rf_model.getNumTrees}\")\n",
    "print(f\"Max Depth: {rf_model.getMaxDepth()}\")\n",
    "print(f\"Min Instances per Node: {rf_model.getMinInstancesPerNode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.8311193160437121\n",
      "RMSE: 234101.61370698604\n"
     ]
    }
   ],
   "source": [
    "# Оценка модели на тестовой выборке\n",
    "predictions = cv_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol='Sales', predictionCol='prediction', metricName='r2')\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "rmse_evaluator = RegressionEvaluator(labelCol='Sales', predictionCol='prediction', metricName='rmse')\n",
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Завершение сессии Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы: <br>\n",
    "На основе датасета Walmart, описывающего пятничные продажи в 45 магазинах соответствующей сети за период в несколько лет, и с помощью библиотеки pyspark составлена модель регрессии (методом случайного леса), позволяющая прогнозировать продажи для каждого из этих магазинов в зависимости от следующих параметров: \n",
    "- совпадение даты с праздничными днями\n",
    "- температура воздуха\n",
    "- стоимость топлива\n",
    "- текущий индекс потребительских цен\n",
    "- текущий уровень безработицы <br>\n",
    "\n",
    "Поиск наилучших гиперпараметров шел c помощью GridSearch на весьма ограниченном наборе гиперпараметров. Впрочем, с учетом того, что данная задача решалась ранее в pandas + sklearn и с применением поиска гиперпараметров с помощью optuna и hyperopt, мы имели представление о более-менее оптимальных значениях гиперпараметров и использовали близкие к ним значения в сетке, по которой pyspark должен был подобрать оптимальные гиперпараметры. <br>\n",
    " В результате наилучшие результаты дала модель с <br>\n",
    "- количеством деревьев: 150, <br>\n",
    "- глубиной леса: 20, <br>\n",
    "- минимальным числом образцов в узле, необходимым для разветвления дерева: 2. <br>\n",
    "При этом объясненная дисперсия составила 0.83 (скромненько), среднеквадратичная ошибка предсказания на тестовом наборе данных 234 тыс. (вообще ужас какой-то). <br>\n",
    "Отметим, что ранее, в случае подбора параметров с помощью optuna и работы в библиотеках pandas + sklearn наилучшими параметрами были: <br> \n",
    "- 'n_estimators': 144, <br>\n",
    "- 'max_depth': 20, <br>\n",
    "- 'min_samples_split': 2, <br>\n",
    "которые дали объясненную дисперсию на уровне 0.999 и среднеквадратичную ошибку предсказания на тестовой выборке порядка 8 тыс. денежных единиц. Это еще раз показывает, что на небольших датасетах лучше не применять pyspark, потому как там отлично работают pandas + sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
