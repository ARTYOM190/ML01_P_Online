{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загрузить файл длиной не менее 2000 символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file, read file, get length, print length, return file content\n",
    "def load_file(filepath: str):\n",
    "    with open(filepath, 'r') as f:\n",
    "        file_content = f.read()\n",
    "        symbols_count = len(file_content)\n",
    "        print(f'File contains {symbols_count} symbols')\n",
    "    \n",
    "    return file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'text-file.txt'\n",
    "file_text = load_file(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Составить программу, которая считает число уникальных слов в тексте (без критерия\n",
    "схожести)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to exclude non-akphabetic and non-spaces symbols \n",
    "from string import punctuation\n",
    "\n",
    "# check that symbol is not punctuation sign\n",
    "def is_target_symbol(symbol: str):\n",
    "    return symbol not in punctuation\n",
    "\n",
    "# exclude unnecessary symbols from text, split to words, make set of words, get set length\n",
    "def calc_unique_words(text: str):\n",
    "    words = ''.join(filter(is_target_symbol, text.lower())).split()\n",
    "    uniques_count = len(set(words))\n",
    "    print(f'There are {uniques_count} unique words in text')\n",
    "    \n",
    "    return uniques_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_count = calc_unique_words(file_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Составить программу, которая считает число гласных и согласных букв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through string and check if symbol in vowels string\n",
    "# if symbol is non-vowel but alphabetic - it's consonant\n",
    "def calculate_letters(string: str):\n",
    "    vowels = 'aeiou'\n",
    "    vowels_count, consonants_count = 0, 0\n",
    "    \n",
    "    for symbol in string:\n",
    "        if symbol in vowels or symbol.lower() in vowels:\n",
    "            vowels_count += 1\n",
    "        elif symbol.isalpha():\n",
    "            consonants_count += 1\n",
    "    \n",
    "    return vowels_count, consonants_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels, consonants = calculate_letters(file_text)\n",
    "print(f'Vowels count - {vowels}, consonats count - {consonants}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Составить программу, которая считает число предложений, их длину и число (количество)\n",
    "раз использования каждого слова в тексте (с критерием схожести, критерий схожести слов\n",
    "выбрать самостоятельно, например, spacy (en_core_web_sm) или расстояние\n",
    "Левенштейна)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета предложений в тексте убираются точки из распространенных сокращений \"Mrs.\", \"Mr.\" Заменяются другие знаки окончания предложения (\"!\", \"?\") на точку для упрощения разделения текста на предложения. Текст режется на предложения по разделителю \".\", предложения делятся на слова. Для предложений, содержащих слова, выводится кол-во слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate count of sentences and words in each one\n",
    "def proc_sentences(string: str):\n",
    "    # replace reduction to exclide point\n",
    "    for reduction in ['Mrs.', 'Mr.']:\n",
    "        proc_str = string.replace(reduction, reduction[:-1])\n",
    "    \n",
    "    # replace other sentences separators to simplify spliting by point\n",
    "    for separator in ['!', '?']:\n",
    "        proc_str = proc_str.replace(separator, '.')\n",
    "        \n",
    "    sentences_list = proc_str.split('.')\n",
    "    sentences_count = 0\n",
    "    \n",
    "    for item in sentences_list:\n",
    "        item = item.strip()\n",
    "        \n",
    "        if item:        \n",
    "            sentences_count += 1\n",
    "            sentence_length = len(item.split())\n",
    "            print('Sentence:\\n', item, '\\nWords count -', sentence_length, '\\n\\n')\n",
    "    \n",
    "    print('Sentences count -', sentences_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета уникальных слов в тексте с критерием схожести сначала убираются знаки пунктуации. Текст (в нижнем регистре) разделяется на токены с помощью библиотеки spacy (легкая моделька en_core_web_sm). Составляется словарь: в качестве ключей - слова в первоначальной форме, значений - кол-ва вхождений слов-ключей в текст. Проверяется выделение символов '\\n', '\\t', '\\s' в отдельный токен с последующем его игнорированием. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation, whitespace\n",
    "import time\n",
    "import spacy\n",
    "\n",
    "# find unique words count with spacy\n",
    "def spacy_algorithm(text: str):\n",
    "    time1 = time.time()\n",
    "\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "    npl = spacy.load('en_core_web_sm')\n",
    "    # replace punctuation symbols in lowercase text, create tokens from text\n",
    "    tokens = npl(text.lower().translate(translator))\n",
    "\n",
    "    unique_text_words = dict()\n",
    "    # fill dictionary with unique words, if word is't in word separators\n",
    "    for token in tokens:\n",
    "        if token.text not in whitespace:\n",
    "            unique_text_words[token.lemma_] = unique_text_words[token.lemma_] + 1 if token.lemma_ in unique_text_words.keys() else 1\n",
    "\n",
    "    unique_text_words = dict(sorted(unique_text_words.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(f'Unique words count is {len(unique_text_words)}')\n",
    "\n",
    "    print('Words used in text:')\n",
    "    for item in unique_text_words.items():\n",
    "        print(item)\n",
    "    \n",
    "    # get method time complexity in ms\n",
    "    print('Time complexity', (time.time() - time1) * 1000, 'ms')\n",
    "    return unique_text_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_text = load_file(filepath)\n",
    "# proc_sentences(file_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text = load_file(filepath)\n",
    "spacy_unique_words = spacy_algorithm(file_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, среди часто встречаемых в тексте много слоВ, не несущих смысловой нагрузки, удаялем их из первой десятки часто встречаемых в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_COUNT = 10\n",
    "\n",
    "top_counter = 0\n",
    "keys_to_delete = list()\n",
    "\n",
    "print(TOP_COUNT, 'words frequently used in text:')\n",
    "# check word key in dictionary is in stop words, append stop words to list, than remove from dictionary\n",
    "for word_key in spacy_unique_words.keys():\n",
    "    if word_key in spacy.lang.en.stop_words.STOP_WORDS:\n",
    "        keys_to_delete.append(word_key)\n",
    "        continue\n",
    "    \n",
    "    top_counter += 1\n",
    "    print(word_key, spacy_unique_words[word_key])\n",
    "    if top_counter == TOP_COUNT:\n",
    "        break\n",
    "\n",
    "for del_key in keys_to_delete:\n",
    "    spacy_unique_words.pop(del_key)\n",
    "\n",
    "print(len(keys_to_delete), 'stop words were excluded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
