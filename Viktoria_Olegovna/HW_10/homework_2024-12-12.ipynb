{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Для 1 любого видео восстановить траекторию движения (t-вектор). Выполнить визуализацию. Определить параметры которые влияют на \"точность\" определения вектора t. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм:\n",
    "1. Считываем по кадру из видео, каждый кадр преобразуем в черно-белый и уменьшаем в 9 раз.\n",
    "2. Получаем особые точки и дескрипторы точек кадра.\n",
    "3. Сопоставляем точки с двух соседних кадров по их дискрипторам.\n",
    "3. Оставляем 20% точек для повышения точности.\n",
    "4. Получаем оценочную матрицу по точкам двух кадров.\n",
    "5. Сохраяем матрицы поворота и смемщения камеры.\n",
    "6. Ключевые точки и дескрипторы текущего кадра сохраняем для дальнейших вычислений на следующей итерации.\n",
    "7. Визуализирум траекторию положений камеры и ее направления в каждой точке.\n",
    "8. Сохраняем массивы с матрицами углов и смещений в npz-файл, т к очередное получение данных матриц занимает много времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as gr\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# function for create camera trajectory by video\n",
    "def process_video(source_video: str):\n",
    "    cap = cv2.VideoCapture(source_video)\n",
    "\n",
    "    # resize frames to speed up simillar points detection \n",
    "    scale = 3\n",
    "    scale_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) / scale)\n",
    "    scale_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) / scale)\n",
    "\n",
    "    # create SIFT detector and BFMatcher objects for all video frames\n",
    "    sift_detector = cv2.SIFT_create()\n",
    "    matcher = cv2.BFMatcher_create(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "    # keypoints and descriptors from prev frame\n",
    "    kps1, des1 = None, None\n",
    "\n",
    "    # create_array with points of camera trajectory\n",
    "    trajectory = np.array([[0, 0, 0]])\n",
    "\n",
    "    # create general list with rotations matrix corresponds to each video frame\n",
    "    rotations_list = [np.zeros((3, 3))]\n",
    "\n",
    "    cam_matrix = np.eye(4)\n",
    "    T = np.eye(4)\n",
    "\n",
    "    while True:\n",
    "        is_success, frame = cap.read()\n",
    "\n",
    "        if is_success:\n",
    "            # convert frame to gray and scale\n",
    "            scaled_frame = cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY), \n",
    "                                      (scale_width, scale_height), cv2.INTER_CUBIC)\n",
    "            # detect frame keypoints\n",
    "            kps2, des2 = sift_detector.detectAndCompute(scaled_frame, None)\n",
    "\n",
    "            if kps1 is not None:\n",
    "                # match keypoints from 2 frames, get 20% from matched points for accuracy\n",
    "                matches = matcher.match(np.array(des1, dtype=np.uint8), np.array(des2, dtype=np.uint8))\n",
    "                matches = sorted(matches, key=lambda x: x.distance)[:int(0.2 * len(matches))]\n",
    "\n",
    "                points1 = np.array([kps1[m.queryIdx].pt for m in matches])\n",
    "                points2 = np.array([kps2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "                # calculate essential matrix to match camera positons between 2 frames\n",
    "                e_mat, mask = cv2.findEssentialMat(points1, points2, method=cv2.LMEDS)\n",
    "                _, R, t, _ = cv2.recoverPose(e_mat, points1, points2, mask=mask)\n",
    "\n",
    "                T[:3, :3] = R\n",
    "                T[:3, 3] = t.T\n",
    "\n",
    "                cam_matrix = np.dot(cam_matrix, T)\n",
    "                trajectory = np.vstack([trajectory, cam_matrix[:3, 3]])\n",
    "                rotations_list.append(cam_matrix[:3, :3])\n",
    "\n",
    "            kps1 = kps2\n",
    "            des1 = des2\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return rotations_list, trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for vizualize cemera trajectory and direction\n",
    "def visualize_trajectory(trajectory, rotation):\n",
    "    fig = gr.Figure()\n",
    "\n",
    "    # add trajectory trace\n",
    "    fig.add_trace(gr.Scatter3d(x=trajectory[:, 0], y=trajectory[:, 1], \n",
    "                               z=trajectory[:, 2],\n",
    "                               marker=dict(size=3, color='purple')))\n",
    "\n",
    "    # add camera orientation traces\n",
    "    for (t, r) in zip(trajectory, rotation):\n",
    "        point2 = t + r[:, 2]\n",
    "        fig.add_trace(gr.Scatter3d(x=[t[0], point2[0]],\n",
    "                                   y=[t[1], point2[1]],\n",
    "                                   z=[t[2], point2[2]],\n",
    "                                   mode='lines',\n",
    "                                   line=dict(width=2, color='red')))\n",
    "\n",
    "    fig.update_layout(title='Camera motion', showlegend=False)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_video = '/media/vik/SamsungSSD7/Courses/peleng-cources/HW_10/videos/concat_video.mp4'\n",
    "\n",
    "# get camera rotations and trajectory\n",
    "rotations_list, trajectory = process_video(source_video)\n",
    "\n",
    "# visualize camera motion\n",
    "visualize_trajectory(trajectory, rotations_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_filename = '/media/vika/SamsungSSD7/Courses/peleng-cources/HW_10/videos/video_data.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save camera rotations and trajectory to npz-file\n",
    "np.savez(npz_filename, R=rotations_list, trajectory=trajectory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load arrays from npz-file\n",
    "data = np.load(npz_filename)\n",
    "trajectory = data['trajectory']\n",
    "rotations_list = data['R']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Использовать решение на базе нейронных сетей. Любые идеи. \n",
    "3. ***slam прикрутить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
