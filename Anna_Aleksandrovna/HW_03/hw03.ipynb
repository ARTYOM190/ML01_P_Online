{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загрузить файл длиной не менее 2000 символов.\n",
    "2. Составить программу, которая считает число уникальных слов в тексте (без критерия схожести)\n",
    "3. Составить программу, которая считает число гласных и согласных букв.\n",
    "4. Составить программу, которая считает число предложений, их длину (количество слов в предложении) и число (количество) раз использования каждого слова в тексте (с критерием схожести, критерий схожести слов выбрать самостоятельно, например, spacy (en_core_web_sm) или расстояние Левенштейна).\n",
    "5. Вывести 10 наиболее часто встречаемых слов.\n",
    "p.s. Рекомендую перед решением задания проанализировать задачу и обосновать алгоритм ее решения в текстовом виде. В процессе написания кода использовать комментарии.\n",
    "6. Используйте стоп слова, приводите текст к одному регистру, удаляйте знаки препинания. Стоп слова были тут раньше spacy.lang.en.stop_words.STOP_WORDS например. Все знаки пунктуации можно найти например тут from string import punctuation.\n",
    "7. текст в файле должен быть на английском языке "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re: Импортируем модуль re, который предоставляет функции для работы с регулярными выражениями, позволяя искать и манипулировать текстом.\n",
    "from collections import Counter: Импортируем класс Counter из модуля collections. Он позволяет подсчитывать количество повторений элементов в коллекции (например, в списке).\n",
    "import string: Импортируем модуль string, который содержит константы и функции для работы со строками (например, наборы символов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем множество (set) с английскими стоп-словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \n",
    "    \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \n",
    "    \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \n",
    "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \n",
    "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \n",
    "    \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \n",
    "    \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
    "    \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \n",
    "    \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \n",
    "    \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \n",
    "    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \n",
    "    \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем ф-ю load_text, которая принимает один параметр file_path. Эта функция предназначена для загрузки текста из нашего текстового файла.\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:- Открываем файл по указанному пути file_path в режиме чтения ('r') с кодировкой UTF-8. Используя with, файл будет автоматически закрыт после завершения работы с ним.\n",
    "text = file.read() - Читаем все содержимое файла и сохраняем его в переменной text.\n",
    "return text - Возвращаем загруженный текст, который теперь доступен для дальнейшей обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ф-я preprocess_text  принимает текст в качестве параметра. Эта функция будет обрабатывать текст для удаления пунктуации и приведения к нижнему регистру.\n",
    "\n",
    "text = text.translate(str.maketrans('', '', string.punctuation)) - тут мы удаляем все знаки пунктуации из текста.\n",
    "\n",
    "str.maketrans('', '', string.punctuation) создает переводчик, который заменяет все знаки пунктуации на пустую строку. translate применяет этот переводчик к тексту.\n",
    "\n",
    "text = text.lower() - Приводим текст к нижнему регистру, чтобы \"Собака\" и \"собака\" считались одним и тем же словом.\n",
    "\n",
    "words = text.split(): Разбиваем текст на отдельные слова, используя пробелы в качестве разделителей. Результат сохраняем в списке words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Удаляем знаки пунктуации\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Приводим текст к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Разбиваем текст на слова\n",
    "    words = text.split()\n",
    "    return words  # Возвращаем все слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ф-я remove_stop_words принимает список слов и будет удалять из него стоп-слова.\n",
    "\n",
    "filtered_words = [word for word in words if word not in stop_words] - Создаем новый список filtered_words, в который добавляем только те слова, которые не являются стоп-словами. Это делаем с помощью генератора списка.\n",
    "\n",
    "return filtered_words - Возвращаем список слов без стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    \"\"\"Удаляет стоп-слова из списка слов и возвращает новый список без стоп-слов.\"\"\"\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words  # Возвращаем слова без стоп-слов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ф-я count_unique_words принимает список слов и подсчитывает количество уникальных слов.\n",
    "\n",
    "unique_words = set(words) - Преобразуем список words в множество unique_words. Множество автоматически удаляет все дубликаты, оставляя только уникальные слова.\n",
    "\n",
    "Возвращаем количество уникальных слов, используя len(), чтобы получить длину множества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words(words):\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ф-я count_vowels_and_consonants будет подсчитывать количество гласных и согласных букв в тексте.\n",
    "\n",
    "vowels = \"aeiou\" - Определяем строку с гласными буквами.\n",
    "consonants = \"bcdfghjklmnpqrstvwxyz\" - Определяем строку с согласными буквами.\n",
    "\n",
    "vowel_count = sum(1 for char in text if char in vowels) - Считаем количество гласных букв в тексте. Используем генератор, который добавляет 1 для каждой гласной буквы, найденной в тексте. sum() суммирует все единицы, полученные от генератора.\n",
    "\n",
    "consonant_count = sum(1 for char in text if char in consonants) - Аналогично, считаем количество согласных букв в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vowels_and_consonants(text):\n",
    "    vowels = \"aeiou\"\n",
    "    consonants = \"bcdfghjklmnpqrstvwxyz\"\n",
    "    vowel_count = sum(1 for char in text if char in vowels)\n",
    "    consonant_count = sum(1 for char in text if char in consonants)\n",
    "    return vowel_count, consonant_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ф-я analyze_sentences_and_word_usage будет анализировать предложения в тексте и подсчитывать частоту использования слов.\n",
    "\n",
    "sentences = re.split(r'[.!?]+', text.strip()) - Используем регулярное выражение для разделения текста на предложения. strip() удаляет пробелы в начале и конце текста. Мы ищем символы окончания предложения (точка, восклицательный и вопросительный знак).\n",
    "\n",
    "sentence_count = len(sentences) - Подсчитываем количество предложений, используя len().\n",
    "\n",
    "word_counts = Counter(re.findall(r'\\b\\w+\\b', text.lower())) - Считаем частоту использования слов в тексте. re.findall(r'\\b\\w+\\b', text.lower()) находит все слова в тексте (игнорируя регистр) и возвращает их в виде списка. Затем Counter подсчитывает, сколько раз каждое слово встречается.\n",
    "\n",
    "sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence] - Создаем список sentence_lengths, который содержит длину каждого предложения (количество слов в нем). Используем генератор списка, который проверяет, что предложение не пустое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentences_and_word_usage(text):\n",
    "    sentences = re.split(r'[.!?]+', text.strip())\n",
    "    sentence_count = len(sentences)\n",
    "    word_counts = Counter(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence]\n",
    "    return sentence_count, sentence_lengths, word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ф-я count_stop_words будет подсчитывать количество повторяющихся стоп-слов.\n",
    "\n",
    "stop_word_counts = Counter(word for word in words if word in stop_words) - Создаем счетчик stop_word_counts, который подсчитывает количество всех стоп-слов в списке words. Используем генератор, который добавляет в счетчик только те слова, которые есть в списке стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stop_words(words):\n",
    "    \"\"\"Подсчитывает количество повторяющихся стоп-слов.\"\"\"\n",
    "    stop_word_counts = Counter(word for word in words if word in stop_words)\n",
    "    return stop_word_counts  # Возвращаем счетчик стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ф-я top_10_words принимает счетчик слов и возвращает 10 наиболее часто встречаемых слов.\n",
    "\n",
    "return word_counts.most_common(10) - Используем метод most_common(10) у счетчика, чтобы получить 10 самых популярных слов и их частоту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_words(word_counts):\n",
    "    return word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\": - Это условие проверяет, запущен ли файл как основная программа. Если да, тогда выполняется весь код под ним.\n",
    "\n",
    "Указываем путь к файлу, который будет содержать текст для анализа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text without stop words:\n",
      "core faroff galaxy billion lightyears earth billion years ago accumulated dense agglomerate gas hundreds millions stars agglomerate gradually shrank one star another flung remaining 100 million stars sank closer center 100 million years agglomerate shrunk several lightyears size small stars began occasionally collide coalesce forming larger stars larger stars consumed fuel imploded form black holes pairs holes flying close occasionally captured orbit around figure 101 shows embedding diagram one blackhole binary hole creates deep pit strong spacetime curvature embedded surface holes encircle orbiting pits produce ripples curvature propagate outward speed light ripples form spiral fabric spacetime around binary much like spiraling pattern water rapidly rotating lawn sprinkler drop water sprinkler flies nearly radially outward bit curvature flies nearly radially outward outward flying drops together form spiraling stream water bits curvature together form spiraling ridges valleys fabric spacetime since spacetime curvature thing gravity ripples curvature actually waves gravity gravitational waves einstein’s general theory relativity predicts unequivocally gravitational waves must produced whenever two black holes orbit other—and also whenever two stars orbit depart outer space gravitational waves push back holes much way bullet kicks back gun fires waves’ push drives holes closer together higher speeds makes slowly spiral inward toward inspiral gradually releases gravitational energy half released energy going waves half increasing holes’ orbital speeds\n",
      "The number of unique words: 141\n",
      "vowel: 667, consonant: 1103\n",
      "Number of offers: 15\n",
      "Length of sentences: [30, 23, 25, 30, 2, 10, 35, 26, 53, 21, 28, 30, 24, 26]\n",
      "The number of repeated stop words:\n",
      "in: 6 \n",
      "the: 30 \n",
      "of: 18 \n",
      "a: 9 \n",
      "from: 3 \n",
      "and: 12 \n",
      "there: 1 \n",
      "as: 7 \n",
      "after: 2 \n",
      "was: 1 \n",
      "out: 1 \n",
      "to: 6 \n",
      "had: 1 \n",
      "their: 1 \n",
      "then: 1 \n",
      "each: 9 \n",
      "other: 6 \n",
      "were: 1 \n",
      "into: 3 \n",
      "an: 1 \n",
      "for: 2 \n",
      "such: 2 \n",
      "that: 4 \n",
      "with: 2 \n",
      "just: 2 \n",
      "so: 2 \n",
      "all: 1 \n",
      "is: 2 \n",
      "same: 2 \n",
      "these: 1 \n",
      "are: 1 \n",
      "or: 1 \n",
      "be: 1 \n",
      "they: 1 \n",
      "on: 2 \n",
      "it: 2 \n",
      "up: 1 \n",
      "them: 1 \n",
      "The 10 most common words:\n",
      "the: 30\n",
      "of: 18\n",
      "and: 13\n",
      "a: 9\n",
      "each: 9\n",
      "as: 7\n",
      "holes: 7\n",
      "other: 7\n",
      "in: 6\n",
      "stars: 6\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = 'text.txt'\n",
    "    text = load_text(file_path)\n",
    "\n",
    "    if len(text) < 2000:\n",
    "        print(\"The text in the file must contain at least 2000 characters\")\n",
    "    else:\n",
    "        words = preprocess_text(text)  # Получаем все слова\n",
    "        filtered_words = remove_stop_words(words)  # Убираем стоп-слова\n",
    "        \n",
    "        # Объединяем обработанные слова обратно в текст\n",
    "        new_text = ' '.join(filtered_words)\n",
    "        \n",
    "        # Выводим новый текст без стоп-слов\n",
    "        print(\"text without stop words:\")\n",
    "        print(new_text)\n",
    "\n",
    "        unique_word_count = count_unique_words(filtered_words)\n",
    "        print(f'The number of unique words: {unique_word_count}')\n",
    "\n",
    "        vowel_count, consonant_count = count_vowels_and_consonants(text)\n",
    "        print(f'vowel: {vowel_count}, consonant: {consonant_count}')\n",
    "\n",
    "        sentence_count, sentence_lengths, word_counts = analyze_sentences_and_word_usage(text)\n",
    "        print(f'Number of offers: {sentence_count}')\n",
    "        print(f'Length of sentences: {sentence_lengths}')\n",
    "\n",
    "        # Подсчет повторяющихся стоп-слов\n",
    "        stop_word_counts = count_stop_words(words)\n",
    "        print('The number of repeated stop words:')\n",
    "        for word, count in stop_word_counts.items():\n",
    "            print(f'{word}: {count} ')\n",
    "\n",
    "        most_common_words = top_10_words(word_counts)\n",
    "        print('The 10 most common words:')\n",
    "        for word, count in most_common_words:\n",
    "            print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОД:\n",
    "Эта программа предназначена для комплексного анализа текстового файла, содержащего английский текст. Она загружает содержимое файла, выполняет предварительную обработку, удаляя знаки препинания и преобразуя текст в нижний регистр, а затем разбивает его на отдельные слова. Программа фильтрует стоп-слова, подсчитывает количество уникальных слов, а также количество гласных и согласных букв в оригинальном тексте. Она анализирует предложения, определяя общее их количество и длину, подсчитывает повторяющиеся стоп-слова и выявляет десять наиболее распространенных слов. Также программа проверяет, содержит ли текст не менее 2000 символов, прежде чем начать анализ. В целом, целью этой программы является углубленный анализ текстового содержания, что может быть полезно в таких областях, как лингвистика, анализ содержания и обработка естественного языка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
