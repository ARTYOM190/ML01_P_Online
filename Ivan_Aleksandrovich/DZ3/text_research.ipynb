{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание:\n",
    "1.\tЗагрузить файл длиной не менее 2000 символов. \n",
    "2.\tСоставить программу, которая считает число уникальных слов в тексте (без критерия схожести)\n",
    "3.\tСоставить программу, которая считает число гласных и согласных букв. \n",
    "4.\tСоставить программу, которая считает число предложений, их длину и число (количество) раз использования каждого слова в тексте (с критерием схожести, критерий схожести слов выбрать самостоятельно, например, spacy (en_core_web_sm) или расстояние Левенштейна). \n",
    "5.\tВывести 10 наиболее часто встречаемых слов. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этапы решения задачи:\n",
    "1. Загрузить текстовый файл с помощью open() и file.read().\n",
    "2. Разделить текст на слова, убрать знаки препинания и привести все символы к нижнему регистру.\n",
    "3. Добавить слова в set, чтобы посчитать количество уникальных слов.\n",
    "4. Создать списки гласных и согласных букв с помощью регулярных выражений, посчитать количество букв в списке.\n",
    "5. Загрузить модуль spacy для работы с предложениями.\n",
    "6. Посчитать длину и количество предложений с помощью spacy.\n",
    "7. Выделить леммы слов с помощью spacy, посчитать входимость каждой леммы в текст.\n",
    "8. Убрать стоп-слова с помощью spacy, посчитать входимость слов в текст.\n",
    "9. Вывести 10 наиболее встречаемых уникальных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем отрывок текста из файла text.txt\n",
    "import os\n",
    "file = open('text.txt', 'r')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отделям слова и убираем знаки препинания, а также приводим текст к нижнему регистру с помощью библиотеки spacy\n",
    "import spacy\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "punctuation = list(punctuation)\n",
    "words = [word.text for sentence in doc.sents for word in sentence \n",
    "               if word.lower_ not in punctuation and\n",
    "               word.pos_ not in ('SPACE','SYM','X')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов (без критерия схожести): 197\n"
     ]
    }
   ],
   "source": [
    "# Выделям уникальные слова посредством добавления их в set\n",
    "uniq_words = set(words)\n",
    "print (\"Количество уникальных слов (без критерия схожести):\", len(uniq_words)) # Выводим количество уникальных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество гласных: 609\n",
      "Количество согласных: 1033\n"
     ]
    }
   ],
   "source": [
    "# Найдем и выведем количество гласных и согласных букв в тексте с помощью регулярных выражений\n",
    "import re\n",
    "vowels = re.findall(r'[aeiouAEIOU]', text)\n",
    "consonants = re.findall(r'[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]', text)\n",
    "print (\"Количество гласных:\", len(vowels))\n",
    "print (\"Количество согласных:\", len(consonants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Номер предложения: 1  Длина предложения:  23\n",
      "Номер предложения: 2  Длина предложения:  48\n",
      "Номер предложения: 3  Длина предложения:  13\n",
      "Номер предложения: 4  Длина предложения:  35\n",
      "Номер предложения: 5  Длина предложения:  8\n",
      "Номер предложения: 6  Длина предложения:  22\n",
      "Номер предложения: 7  Длина предложения:  5\n",
      "Номер предложения: 8  Длина предложения:  10\n",
      "Номер предложения: 9  Длина предложения:  8\n",
      "Номер предложения: 10  Длина предложения:  45\n",
      "Номер предложения: 11  Длина предложения:  54\n",
      "Номер предложения: 12  Длина предложения:  94\n",
      "В тексте 12 предложений\n"
     ]
    }
   ],
   "source": [
    "#Реализуем подсчет количества и длины предложений с помощью spacy\n",
    "counter = 1\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(\"Номер предложения:\" , counter, \" Длина предложения: \", len(sentence.text.split(' ')))\n",
    "    counter +=1\n",
    "print (\"В тексте\", counter-1, \"предложений\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета наиболее встречаемых слов в тексте с учетом схожести также применим spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 наиболее часто встречаемых слов в тексте: \n",
      "the - 20\n",
      "of - 18\n",
      "was - 14\n",
      "a - 12\n",
      "in - 11\n",
      "to - 10\n",
      "and - 8\n",
      "that - 6\n",
      "be - 5\n",
      "it - 5\n"
     ]
    }
   ],
   "source": [
    "# Извлекаем слова, добавляем их в словарь и считаем количество вхождений\n",
    "doc = nlp(' '.join(words))\n",
    "tokens = {}\n",
    "for token in doc:\n",
    "    if token.text in tokens.keys():\n",
    "        tokens[token.text] += 1\n",
    "    else:\n",
    "        tokens[token.text] = 1\n",
    "# Сортируем словарь по количеству вхождений и выводим 10 наиболее часто встречаемых слов в тексте\n",
    "sorted_tokens = sorted(tokens.items(), key= lambda item: item[1],reverse=1)\n",
    "print (\"10 наиболее часто встречаемых слов в тексте: \")\n",
    "for item in sorted_tokens[0:10]:\n",
    "    print (item[0], \"-\", item[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 наиболее часто встречаемых слов в тексте с учетом лемматизации и убранных стоп-слов:\n",
      "Jane - 4\n",
      "wife - 3\n",
      "Mr. - 3\n",
      "Bingley - 3\n",
      "visit - 3\n",
      "manner - 3\n",
      "truth - 2\n",
      "man - 2\n",
      "consider - 2\n",
      "daughter - 2\n",
      "Количество уникальных слов:  118\n",
      "Процент уникальных слов:  7.02439024390244\n"
     ]
    }
   ],
   "source": [
    "# Также рассчитаем количество наиболее часто встречаемых слов в тексте с учетом лемматизации и убранных стоп-слов.\n",
    "from spacy.lang.en import stop_words\n",
    "\n",
    "stop_words = stop_words.STOP_WORDS\n",
    "doc = nlp(text)\n",
    "tokens = {}\n",
    "# Создаем список слов, не включающий стоп-слова и знаки пунктуации\n",
    "words = [word.lemma_ for sentence in doc.sents for word in sentence \n",
    "               if word.pos_ not in ('PRON','SPACE','SYM','X') and \n",
    "                  word.lower_ not in stop_words and \n",
    "                  word.lower_ not in punctuation]\n",
    "for word in words:\n",
    "    if word in tokens.keys():\n",
    "        tokens[word] += 1\n",
    "    else:\n",
    "        tokens[word] = 1\n",
    "        \n",
    "sorted_tokens = sorted(tokens.items(), key= lambda item: item[1],reverse=1)\n",
    "print (\"10 наиболее часто встречаемых слов в тексте с учетом лемматизации и убранных стоп-слов:\")\n",
    "for item in sorted_tokens[0:10]:\n",
    "    print (item[0], \"-\", item[1])\n",
    "print (\"Количество уникальных слов: \", len(sorted_tokens))\n",
    "                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "1. Количество согласных букв в тексте примерно в 2 раза больше чем гласных.\n",
    "2. Наиболее часто встречаемые слова в тексте являются местоимениями и предлогами и не несут смысловой нагрузки.\n",
    "3. Лемматизация позволяет привести слова к начальной форме и исключить повторения различных форм одного слова."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DZ3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
