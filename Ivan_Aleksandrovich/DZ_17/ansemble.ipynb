{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнить Стэкинг, Бэгинг, Вотинг и Бустинг. При реализации алгоритмов не использовать готовые решения. \n",
    "За сравнение взять CatBoostClassifier как базовая метрика качества. Сравнить результат с реализацией своих ансамблей. \n",
    "Для однозначности и интерпретируемости результатов использовать приложенный набор данных. \n",
    "При реализации бустинга - просто сокращайте набор данных на котором модель отработала хорошо (правильно предсказанные данные). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error, accuracy_score, f1_score\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "data = pd.read_csv('winequality-white.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на соотношение классов в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процентное соотношение классов (quality):\n",
      "Класс 3: 0.41%\n",
      "Класс 4: 3.33%\n",
      "Класс 5: 29.75%\n",
      "Класс 6: 44.88%\n",
      "Класс 7: 17.97%\n",
      "Класс 8: 3.57%\n",
      "Класс 9: 0.10%\n"
     ]
    }
   ],
   "source": [
    "# Считаем количество каждого класса\n",
    "class_counts = data['quality'].value_counts().sort_index()\n",
    "\n",
    "# Считаем процентное соотношение\n",
    "class_percentages = (class_counts / class_counts.sum()) * 100\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Процентное соотношение классов (quality):\")\n",
    "for quality, percentage in class_percentages.items():\n",
    "    print(f\"Класс {quality}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированы. Применим RandomOverSampler для балансировки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['quality']\n",
    "X = data.drop('quality', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Разделение данных на тренировочную и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем RandomOverSampler к тренировочным данным\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Масштабируем признаки\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим и проверим CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект Pool\n",
    "train_pool = Pool(X_train_scaled, y_train)\n",
    "test_pool = Pool(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    \"iterations\": hp.choice(\"iterations\", range(100, 2000)),  # Количество деревьев\n",
    "    \"depth\": hp.choice(\"depth\", range(3, 10)),  # Глубина деревьев\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", -3, 0),  # Темп обучения (0.001 - 1)\n",
    "    \"l2_leaf_reg\": hp.uniform(\"l2_leaf_reg\", 1, 10),  # Регуляризация L2\n",
    "    \"border_count\": hp.choice(\"border_count\", range(32, 255)),  # Количество бинов разбиения\n",
    "    \"bagging_temperature\": hp.uniform(\"bagging_temperature\", 0, 1),  # Температура бэггинга\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=int(params[\"iterations\"]),\n",
    "        depth=int(params[\"depth\"]),\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        l2_leaf_reg=params[\"l2_leaf_reg\"],\n",
    "        border_count=int(params[\"border_count\"]),\n",
    "        bagging_temperature=params[\"bagging_temperature\"],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_scaled, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "    return {\"loss\": -accuracy, \"status\": STATUS_OK} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:21<00:00,  4.03s/trial, best loss: -0.6534166160595409]\n",
      "Лучшие параметры: {'bagging_temperature': 0.6204685008777656, 'border_count': 103, 'depth': 6, 'iterations': 1763, 'l2_leaf_reg': 1.9900607052193315, 'learning_rate': 0.15275517180137332}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trials = Trials()\n",
    "best_params = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,  # Количество итераций оптимизации\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = CatBoostClassifier(**best_params)\n",
    "model.fit(train_pool, eval_set=test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6275510204081632\n"
     ]
    }
   ],
   "source": [
    "# 8. Оценка модели на тестовой выборке\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика accuracy модели CatBoostClassifier - 0.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем bagging, voting, stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bagging(X_train, y_train, X_test, models, n_estimators=10, sample_fraction=0.8, random_state=None):\n",
    "    \"\"\"Бэггинг: усредняет предсказания нескольких моделей, обученных на случайных подвыборках.\"\"\"\n",
    "    n_samples = int(len(X_train) * sample_fraction)\n",
    "    predictions = np.zeros((n_estimators, len(X_test)), dtype=int)\n",
    "\n",
    "    # Фиксируем случайность, если указан random_state\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    for i in range(n_estimators):\n",
    "        idxs = np.random.choice(len(X_train), n_samples, replace=True) # Бутстрэп\n",
    "        X_sample, y_sample = X_train[idxs], y_train.iloc[idxs]\n",
    "        \n",
    "        model = clone(models[i % len(models)])  # Берем модель из списка\n",
    "        model.fit(X_sample, y_sample)\n",
    "        \n",
    "        predictions[i] = model.predict(X_test)\n",
    "\n",
    "    # Мажоритарное голосование\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
    "    \n",
    "    return final_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting(X_train, y_train, X_test, models, voting_type=\"hard\"):\n",
    "    \"\"\"Вотинг: усредняет предсказания моделей (hard – по голосам, soft – по вероятностям).\"\"\"\n",
    "    trained_models = [clone(model).fit(X_train, y_train) for model in models]\n",
    "    predictions = np.array([model.predict(X_test) for model in trained_models])\n",
    "\n",
    "    if voting_type == \"hard\":\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
    "    else:\n",
    "        probs = np.array([model.predict_proba(X_test) for model in trained_models])\n",
    "        avg_probs = np.mean(probs, axis=0)\n",
    "        return np.argmax(avg_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "def stacking(X_train, y_train, X_test, models, meta_model):\n",
    "    \"\"\"Стекинг: обучаем модели, собираем предсказания и обучаем мета-модель\"\"\"\n",
    "    \n",
    "    trained_models = []\n",
    "    \n",
    "    # Обучаем базовые модели и собираем предсказания для мета-признаков\n",
    "    meta_features_train = np.column_stack([\n",
    "        clone(model).fit(X_train, y_train).predict(X_train) for model in models\n",
    "    ])\n",
    "    \n",
    "    # Теперь обучаем сами модели (они понадобятся для теста)\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models.append(model)\n",
    "    \n",
    "    # Предсказываем на тесте\n",
    "    meta_features_test = np.column_stack([model.predict(X_test) for model in trained_models])\n",
    "\n",
    "    # Обучаем мета-модель\n",
    "    meta_model.fit(meta_features_train, y_train)\n",
    "    \n",
    "    return meta_model.predict(meta_features_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем проверять методы на моделях DecisionTreeClassifier, RandomForestClassifier, LogisticRegression (как результирующая модель для Стекинга)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим метрики моделей по отдельности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5827\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6571\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\soft\\Anaconda\\envs\\ML1\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И проверим собственные реализации вотинга, бэггинга и стекинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\soft\\Anaconda\\envs\\ML1\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.6448979591836734\n",
      "Stacking Accuracy: 0.6173469387755102\n",
      "Voting Accuracy: 0.639795918367347\n"
     ]
    }
   ],
   "source": [
    "# Задаем модели\n",
    "models = [DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Вызываем методы\n",
    "y_pred_bagging = bagging(X_train_scaled, y_train, X_test_scaled, models)\n",
    "y_pred_stacking = stacking(X_train_scaled, y_train, X_test_scaled, models, meta_model)\n",
    "y_pred_voting = voting(X_train_scaled, y_train, X_test_scaled, models, voting_type=\"hard\")\n",
    "\n",
    "# Выводим точности\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_bagging))\n",
    "print(\"Stacking Accuracy:\", accuracy_score(y_test, y_pred_stacking))\n",
    "print(\"Voting Accuracy:\", accuracy_score(y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем Бустинг на основе деревьев решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def boosting(X_train, y_train, X_test, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "   \n",
    "    # Инициализация нулевого предсказания (среднее значение таргета)\n",
    "    predictions = np.full_like(y_train, np.mean(y_train), dtype=np.float64)\n",
    "    \n",
    "    models = []  # Список моделей\n",
    "\n",
    "    for _ in range(n_estimators):\n",
    "        # Вычисляем градиент (ошибку между предсказаниями и реальными значениями)\n",
    "        residuals = y_train - predictions\n",
    "\n",
    "        # Обучаем слабый базовый алгоритм (решающее дерево)\n",
    "        model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        model.fit(X_train, residuals)\n",
    "\n",
    "        # Добавляем слабую модель в список\n",
    "        models.append(model)\n",
    "\n",
    "        # Обновляем предсказания, добавляя скорректированные ошибки с учетом learning_rate\n",
    "        predictions += learning_rate * model.predict(X_train)\n",
    "\n",
    "    # Финальное предсказание для тестовой выборки\n",
    "    final_predictions = np.full_like(y_test, np.mean(y_train), dtype=np.float64)\n",
    "    for model in models:\n",
    "        final_predictions += learning_rate * model.predict(X_test)\n",
    "\n",
    "    return final_predictions.round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.6336734693877552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_boosting = boosting(X_train_scaled, y_train, X_test_scaled,n_estimators=5000, learning_rate=0.2)\n",
    "\n",
    "\n",
    "# Выводим точности\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_boosting))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эталонное значение accuracy модели CatBoost - 0.931, подбор параметров выполнен с помощью Hyperopt.\n",
    "При применении ансамблей лучший результат получен при использовании Voting - 0.933, однако он меньше результата CatBoost и меньше чем accuracy при использовании чистого RandomForestClassifier.\n",
    "В моем случае, применение ансамблей усредняет результаты моделей (для Voting и Bagging). При использовании Stacking слабая модель LogisticRegression не сильно влияет на итоговый результат.\n",
    "\n",
    "Реализация бустинга на основе деревьев решений дала результат accuracy 0.928. Результат напрямую зависит от количества базовых моделей (рещающих деревьев)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
