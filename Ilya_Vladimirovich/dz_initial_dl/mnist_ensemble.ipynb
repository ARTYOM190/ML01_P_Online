{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "ДЗ - обучить при помощи tf (keras) API любое ДЗ по обучениею классических моделей. \n",
    "Можно сдавать на торче.\n",
    "\n",
    "### Анализ\n",
    "Интересно построить ансамбль моделей и изучить результат работы.  \n",
    "Для разнообразия моделей активационные функции и кожффициент дропаута будут выбираться случайным образом.  \n",
    "В качестве датасета был выбран MNIST\n",
    "\n",
    "### Имплементация\n",
    "Определим нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "import random\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class CNN_Number_Predictor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN_Number_Predictor, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            self.random_activation(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            self.random_activation(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            self.random_activation(),\n",
    "            self.random_dropout(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            self.random_activation(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            self.random_activation(),\n",
    "            nn.Conv2d(64, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            self.random_activation(),\n",
    "            self.random_dropout(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            self.random_activation(),\n",
    "            nn.Flatten(),\n",
    "            self.random_dropout(),\n",
    "            nn.Linear(2048, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    \n",
    "    def random_activation(self):\n",
    "        acts = [\n",
    "                nn.ReLU(),\n",
    "                nn.LeakyReLU(negative_slope=random.choice([0.01, 0.1, 0.3])),\n",
    "                nn.GELU(),\n",
    "                nn.SiLU(),\n",
    "                nn.Mish(),\n",
    "                nn.CELU(alpha=random.uniform(0.1, 1.0))\n",
    "            ]\n",
    "        return random.choice(acts)\n",
    "\n",
    "    def random_dropout(self):\n",
    "        return nn.Dropout(p=random.uniform(0.2, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем информацию о 1 модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN_Number_Predictor                     [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 10]                   --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 28, 28]           320\n",
       "│    └─BatchNorm2d: 2-2                  [1, 32, 28, 28]           64\n",
       "│    └─Mish: 2-3                         [1, 32, 28, 28]           --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 28, 28]           9,248\n",
       "│    └─BatchNorm2d: 2-5                  [1, 32, 28, 28]           64\n",
       "│    └─CELU: 2-6                         [1, 32, 28, 28]           --\n",
       "│    └─Conv2d: 2-7                       [1, 32, 14, 14]           25,632\n",
       "│    └─BatchNorm2d: 2-8                  [1, 32, 14, 14]           64\n",
       "│    └─Mish: 2-9                         [1, 32, 14, 14]           --\n",
       "│    └─Dropout: 2-10                     [1, 32, 14, 14]           --\n",
       "│    └─Conv2d: 2-11                      [1, 64, 14, 14]           18,496\n",
       "│    └─BatchNorm2d: 2-12                 [1, 64, 14, 14]           128\n",
       "│    └─CELU: 2-13                        [1, 64, 14, 14]           --\n",
       "│    └─Conv2d: 2-14                      [1, 64, 14, 14]           36,928\n",
       "│    └─BatchNorm2d: 2-15                 [1, 64, 14, 14]           128\n",
       "│    └─SiLU: 2-16                        [1, 64, 14, 14]           --\n",
       "│    └─Conv2d: 2-17                      [1, 64, 7, 7]             102,464\n",
       "│    └─BatchNorm2d: 2-18                 [1, 64, 7, 7]             128\n",
       "│    └─Mish: 2-19                        [1, 64, 7, 7]             --\n",
       "│    └─Dropout: 2-20                     [1, 64, 7, 7]             --\n",
       "│    └─Conv2d: 2-21                      [1, 128, 4, 4]            131,200\n",
       "│    └─BatchNorm2d: 2-22                 [1, 128, 4, 4]            256\n",
       "│    └─LeakyReLU: 2-23                   [1, 128, 4, 4]            --\n",
       "│    └─Flatten: 2-24                     [1, 2048]                 --\n",
       "│    └─Dropout: 2-25                     [1, 2048]                 --\n",
       "│    └─Linear: 2-26                      [1, 10]                   20,490\n",
       "│    └─Softmax: 2-27                     [1, 10]                   --\n",
       "==========================================================================================\n",
       "Total params: 345,610\n",
       "Trainable params: 345,610\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 30.53\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1.39\n",
       "Params size (MB): 1.38\n",
       "Estimated Total Size (MB): 2.77\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_Number_Predictor().to(device)\n",
    "\n",
    "summary(model, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настроим загрузчики данных и аугментацию. Для аугментации будем испоьлозвать случайные афинные преобразования и стирания части информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2088815.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 15035934.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=(-20, 20), translate=(0.2, 0.2), shear=10, scale=(0.5, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.25),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "training_data = MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "test_data = MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, num_workers=16)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию для обучения 1 модели и функцию тестирования ансамбля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for _ , (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(dataloader, models, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    majority_votes = []\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        predictions = []\n",
    "\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = model(X)\n",
    "                predictions.append(pred)\n",
    "\n",
    "        predictions = torch.stack(predictions)\n",
    "        majority_vote = torch.mode(predictions.argmax(dim=2), dim=0).values\n",
    "        majority_votes.append(majority_vote)\n",
    "\n",
    "        loss = loss_fn(predictions.mean(dim=0), y)\n",
    "        test_loss += loss.item()\n",
    "        correct += (majority_vote == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    return 100 * correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим 32 модели и выведем Accuracy для тестового датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.48\n"
     ]
    }
   ],
   "source": [
    "num_models = 32\n",
    "models = [CNN_Number_Predictor().to(device) for _ in range(num_models)]\n",
    "optimizers = [torch.optim.AdamW(model.parameters()) for model in models]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for _ in range(100):\n",
    "        train(train_dataloader, model, loss_fn, optimizers[i])\n",
    "    torch.save(model.state_dict(), f'ensemble_model_{i}_weights.pth')\n",
    "print(test(test_dataloader, models, loss_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- Обучен ансамбль из 32 моделей\n",
    "- Использована аугментация данных для лучшей обобщающей способности модели\n",
    "- Использован оптимизатор AdamW со штрафом за большие весовые коэффициенты\n",
    "- Accuracy модели 99.48%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
