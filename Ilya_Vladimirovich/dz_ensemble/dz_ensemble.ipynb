{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "Выполнить Стэкинг, Бэгин, Вотинг и Бустинг. При реализации алгоритмов не использовать готовые решения. \n",
    "За сровнение взять CatBoostClassifier как базовая метрика качества. Сравнить результат с реализацией своих ансамблей. \n",
    "Для однозначности и интерпретируемости результатов использовать приложенный набор данных. \n",
    "При реализации бустинга - просто сокращайте набор данных на котором модель отработала хорошо (правильно предсказанные данные). \n",
    "\n",
    "### Анализ\n",
    "Построим как можно больше базовых моделей, работающих на разных принципах. На их основе составим несколько типов ансамблей. Сравним точность моделей с CatBoost \n",
    "\n",
    "### Имплементация\n",
    "\n",
    "Загрузим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.0              0.27         0.36            20.7      0.045   \n",
      "1            6.3              0.30         0.34             1.6      0.049   \n",
      "2            8.1              0.28         0.40             6.9      0.050   \n",
      "3            7.2              0.23         0.32             8.5      0.058   \n",
      "4            7.2              0.23         0.32             8.5      0.058   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
      "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
      "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
      "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      8.8        6  \n",
      "1      9.5        6  \n",
      "2     10.1        6  \n",
      "3      9.9        6  \n",
      "4      9.9        6  \n",
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
      "mean        6.854788          0.278241     0.334192        6.391415   \n",
      "std         0.843868          0.100795     0.121020        5.072058   \n",
      "min         3.800000          0.080000     0.000000        0.600000   \n",
      "25%         6.300000          0.210000     0.270000        1.700000   \n",
      "50%         6.800000          0.260000     0.320000        5.200000   \n",
      "75%         7.300000          0.320000     0.390000        9.900000   \n",
      "max        14.200000          1.100000     1.660000       65.800000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
      "mean      0.045772            35.308085            138.360657     0.994027   \n",
      "std       0.021848            17.007137             42.498065     0.002991   \n",
      "min       0.009000             2.000000              9.000000     0.987110   \n",
      "25%       0.036000            23.000000            108.000000     0.991723   \n",
      "50%       0.043000            34.000000            134.000000     0.993740   \n",
      "75%       0.050000            46.000000            167.000000     0.996100   \n",
      "max       0.346000           289.000000            440.000000     1.038980   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
      "mean      3.188267     0.489847    10.514267     5.877909  \n",
      "std       0.151001     0.114126     1.230621     0.885639  \n",
      "min       2.720000     0.220000     8.000000     3.000000  \n",
      "25%       3.090000     0.410000     9.500000     5.000000  \n",
      "50%       3.180000     0.470000    10.400000     6.000000  \n",
      "75%       3.280000     0.550000    11.400000     6.000000  \n",
      "max       3.820000     1.080000    14.200000     9.000000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv('winequality-white.csv', sep=';')\n",
    "print(data.head())\n",
    "print(data.describe())\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим и оптимизируем базовые модели для классификации\n",
    "\n",
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cd6041352c4ea98738f8de83ce6a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 26, 'weights': 'distance', 'p': 1}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "N_TRIALS = 1000\n",
    "\n",
    "def objective_knn(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1, 50),\n",
    "        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "        'p': trial.suggest_int('p', 1, 3)\n",
    "    }\n",
    "    model = KNeighborsClassifier(**params)\n",
    "    return cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "study_knn = optuna.create_study(direction='maximize')\n",
    "study_knn.optimize(objective_knn, n_trials=N_TRIALS, show_progress_bar=True, n_jobs=-1)\n",
    "best_knn = KNeighborsClassifier(**study_knn.best_params)\n",
    "\n",
    "print(study_knn.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac32aebc50e14710bea5eff2cdaf8e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 1, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "def objective_tree(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss'])\n",
    "    }\n",
    "    model = DecisionTreeClassifier(**params)\n",
    "    return cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "study_tree = optuna.create_study(direction='maximize')\n",
    "study_tree.optimize(objective_tree, n_trials=N_TRIALS, show_progress_bar=True, n_jobs=-1)\n",
    "best_tree = DecisionTreeClassifier(**study_tree.best_params)\n",
    "\n",
    "print(study_tree.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5409be63af498ab4571cd8f8d9fed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 8.065678596217653, 'kernel': 'rbf', 'gamma': 'auto', 'degree': 5}\n"
     ]
    }
   ],
   "source": [
    "def objective_svm(trial):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid']),\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
    "        'degree': trial.suggest_int('degree', 2, 5)\n",
    "    }\n",
    "    model = SVC(**params, probability=True)\n",
    "    return cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "study_svm = optuna.create_study(direction='maximize')\n",
    "study_svm.optimize(objective_svm, n_trials=N_TRIALS, show_progress_bar=True, n_jobs=-1)\n",
    "best_svm = SVC(**study_svm.best_params, probability=True)\n",
    "\n",
    "print(study_svm.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dc6c7a1aa94bce9ccc1a74cce827c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 176, 'learning_rate': 0.057458558420938666, 'max_depth': 8, 'min_samples_split': 16, 'min_samples_leaf': 6, 'subsample': 0.9308140603990446}\n"
     ]
    }
   ],
   "source": [
    "def objective_gb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    }\n",
    "    model = GradientBoostingClassifier(**params)\n",
    "    return cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "study_gb = optuna.create_study(direction='maximize')\n",
    "study_gb.optimize(objective_gb, n_trials=N_TRIALS / 5, show_progress_bar=True, n_jobs=-1)\n",
    "best_gb = GradientBoostingClassifier(**study_gb.best_params)\n",
    "\n",
    "print(study_gb.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d014b09392746f99df6f5421d2dd562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'solver': 'sgd', 'alpha': 0.004919926382476044, 'learning_rate_init': 0.03717539393212284, 'batch_size': 65, 'hidden_layer_sizes': (885, 893, 221)}\n"
     ]
    }
   ],
   "source": [
    "def objective_mlp(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 6)\n",
    "    layers = [trial.suggest_int(f'n_units_l{i}', 50, 1000) for i in range(n_layers)]\n",
    "    params = {\n",
    "        'hidden_layer_sizes': tuple(layers),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic']),\n",
    "        'solver': trial.suggest_categorical('solver', ['adam', 'sgd']),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 0.05),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 0.001, 0.1),\n",
    "        'batch_size': trial.suggest_int('batch_size', 32, 128)\n",
    "    }\n",
    "    model = MLPClassifier(**params, max_iter=N_TRIALS ** 2)\n",
    "    return cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "study_mlp = optuna.create_study(direction='maximize')\n",
    "study_mlp.optimize(objective_mlp, n_trials=N_TRIALS / 10, show_progress_bar=True)\n",
    "\n",
    "best_params = study_mlp.best_params\n",
    "n_layers = best_params['n_layers']\n",
    "layers = [best_params[f'n_units_l{i}'] for i in range(n_layers)]\n",
    "best_params['hidden_layer_sizes'] = tuple(layers)\n",
    "best_params = {k: v for k, v in best_params.items() if k not in ['n_layers'] + [f'n_units_l{i}' for i in range(n_layers)]}\n",
    "best_mlp = MLPClassifier(**best_params, max_iter=N_TRIALS ** 2)\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим ансамбли моделей\n",
    "\n",
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.base import clone\n",
    "\n",
    "class VotingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [(name, clone(model).fit(X, y)) for name, model in self.models]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for _, model in self.models_]).T\n",
    "        return np.apply_along_axis(\n",
    "            lambda x: np.argmax(np.bincount(x, minlength=len(np.unique(y)))),\n",
    "            axis=1,\n",
    "            arr=predictions.astype(int)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models, n_bags=50):\n",
    "        self.models = models\n",
    "        self.n_bags = n_bags\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        self.bagged_models_ = []\n",
    "        for _ in range(self.n_bags):\n",
    "            indices = np.random.choice(X.shape[0], X.shape[0], replace=True)\n",
    "            bagged_models = [(name, clone(model).fit(X[indices], y[indices])) for name, model in self.models]\n",
    "            self.bagged_models_.append(bagged_models)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        bagged_predictions = []\n",
    "        for bagged_models in self.bagged_models_:\n",
    "            predictions = np.array([model.predict(X) for _, model in bagged_models]).T\n",
    "            bagged_predictions.append(predictions)\n",
    "        bagged_predictions = np.mean(bagged_predictions, axis=0)\n",
    "        return np.round(np.mean(bagged_predictions, axis=1)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models, meta_model):\n",
    "        self.models = models\n",
    "        self.meta_model = meta_model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [(name, clone(model).fit(X, y)) for name, model in self.models]\n",
    "        base_predictions = np.array([model.predict(X) for _, model in self.models_]).T\n",
    "        self.meta_model_ = self.meta_model.fit(base_predictions, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        base_predictions = np.array([model.predict(X) for _, model in self.models_]).T\n",
    "        return self.meta_model_.predict(base_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В качестве референсной модели будем использовать модель CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost accuracy: 0.6693877551020408\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost = CatBoostClassifier(verbose=0, allow_writing_files=False)\n",
    "catboost.fit(X_train, y_train)\n",
    "catboost_accuracy = accuracy_score(y_test, catboost.predict(X_test))\n",
    "\n",
    "print(f\"CatBoost accuracy: {catboost_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выведем метрику точности для всех моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  Accuracy\n",
      "0                KNN  0.663265\n",
      "1      Decision Tree  0.620408\n",
      "2                SVM  0.595918\n",
      "3  Gradient Boosting  0.684694\n",
      "4                MLP  0.644898\n",
      "5             Voting  0.687755\n",
      "6            Bagging  0.682173\n",
      "7           Stacking  0.679592\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('KNN', best_knn),\n",
    "    ('Decision Tree', best_tree),\n",
    "    ('SVM', best_svm),\n",
    "    ('Gradient Boosting', best_gb),\n",
    "    ('MLP', best_mlp)\n",
    "]\n",
    "\n",
    "ensembles = [\n",
    "    ('Voting', VotingClassifier(models)),\n",
    "    ('Bagging', BaggingClassifier(models)),\n",
    "    ('Stacking', StackingClassifier(models, GradientBoostingClassifier()))\n",
    "]\n",
    "\n",
    "all_models = models + ensembles\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for name, model in all_models:\n",
    "    model.fit(X_train, y_train)\n",
    "    scores[name] = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "scores_df = pd.DataFrame(list(scores.items()), columns=['Model', 'Accuracy'])\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- Обучили 5 базовых моделей классификаторов. KNN и GradientBoostingClassifier сравнимы по точности с CatBoostClassifier. Остальные модели по метрикам хуже.\n",
    "- Построили 3 различных ансамбля из базовых моделей. Все ансамбли по метрикам лучше CatBoostClassifier.\n",
    "- На подбор гиперпараметров и обучение моделей с хорошими метриками потребовалось много вычислительных ресурсов. CatBoostClassifier имеет метрики не сильно хуже, однако дает результат из коробки и очень быстро."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
