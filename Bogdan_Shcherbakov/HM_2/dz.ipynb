{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tЗагрузить файл длиной не менее 2000 символов. \n",
    "2.\tСоставить программу, которая считает число уникальных слов в тексте (без критерия схожести)\n",
    "\n",
    "3.\tСоставить программу, которая считает число гласных и согласных букв. \n",
    "4.\tСоставить программу, которая считает число предложений, их длину и число (количество) раз использования каждого слова в тексте (с критерием схожести, критерий схожести слов выбрать самостоятельно, например, spacy (en_core_web_sm) или расстояние Левенштейна). \n",
    "5.\tВывести 10 наиболее часто встречаемых слов. \n",
    "\n",
    "p.s. Рекомендую перед решением задания проанализировать задачу и обосновать алгоритм ее решения в текстовом виде. В процессе написания кода использовать комментарии.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цель задач: научится работать с большим объемом данных, в данном случае текст более 2000 символов.\n",
    "\n",
    "### Решение: для задачи была взята какая-то большая книга. \n",
    "- Первым делом был написан метод txt_reader() который открывает файл и возвращает текст в нижнем регистре, rstrip() почему то не работает. \n",
    "\n",
    "- Далее метод get_clean_txt() прогоняет весь текст через регулярное выражение, которое пропускает только буквы, пробелы, апостроф и тире.\n",
    "\n",
    "- В методе num_of_unic_worlds() разбиваю текст без знаков препинания пробелами с помощью split() и забрасываю все в set длинна которого и есть кол-во уникальных слов.\n",
    "\n",
    "- num_of_consonants() сравниваю строку согласных с текстом считая все совпадения, аналогично в num_of_vowels(), интересный факт что английская буква Y являться как гласной так и согласной буквой.\n",
    "\n",
    "- Для подсчета предложений использовал тот же принцип что и с буквами, только искал знаки препинания конца предложения в num_of_sentences().\n",
    "\n",
    "- Почитав про различные критерии сложности мне показалось что для большого текста использовать их было бы не оптимально в силу быстродействия программы. Изучил основные методы библиотеки spacy (spacy_tokenization(), top_wolds()) где нашел функцию преобразования слова в его базовую форму .lemma_. Преобразовав все слова в их базовую форму закинул их как ключ в словарь и для каждого посчитал количество в тексте. Отсортировав вывел топ 10 самых встречаемых top_wolds().\n",
    "\n",
    "- Для подсчета длинны предложения использовал split() для каждого предложения, решил не выводить все предложения и их длину потому что много, посчитал среднюю длину.\n",
    "\n",
    "### Вывод: \n",
    "Для основных задач написал простые функции. Для подсчета слов с критерием сложности использовал spacy, его работа показалось мне достаточно медленной для большого текста, насколько я понял сам процесс токенизации очень долгий. В нем же нашел метод подсчета предложений что существенно разошелся (окло 20%) с моей функцией, думаю это в силу конструкций Mr.Holland где точка считается за предложение, хотя это является сокращением. Как улучшение своей функции вижу вариант с использованием регулярного выражения где условно берут в учет что предложение начинается с большой буквы, а перед ним пробел и знак конца предложения. Для токенизации текста по методу lemma_ использовал чистый текст т.к. в этом случае нам не нужны знаки препинания.\n",
    "\n",
    "### Результаты выыода программы:\n",
    "\n",
    ">Колличество гласных: 174432\n",
    "\n",
    ">Колличество согласных: 266732\n",
    "\n",
    ">Колличество уникальных слов: 15699\n",
    "\n",
    ">Колличество предложений : 7294\n",
    "\n",
    ">Колличество предложений от spacy : 5679\n",
    "\n",
    ">Средняя длинна предложения 18.55273815812643\n",
    "\n",
    ">Топ 10 часто используемых слов: ['the', 'be', 'I', 'and', 'to', 'of', 'a', 'have', 'he', 'in']\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
