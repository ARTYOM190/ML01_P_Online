{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202eb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torcheval\n",
      "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\антон\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torcheval) (4.12.2)\n",
      "Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: torcheval\n",
      "Successfully installed torcheval-0.0.7\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "#!pip install tensordict\n",
    "#!pip install torchrl\n",
    "#!pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a220559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MobileBertConfig, MobileBertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoTokenizer, MobileBertModel,  BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torchrl.data import TensorDictTokenizer\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torcheval.metrics import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d316b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Размер батча\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7e05fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "data = pd.read_csv('imdb dataset.csv')  \n",
    "data = data[['review', 'sentiment']]  # \n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a74b1924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  label\n",
      "0  One of the other reviewers has mentioned that ...  positive      1\n",
      "1  A wonderful little production. <br /><br />The...  positive      1\n",
      "2  I thought this was a wonderful way to spend ti...  positive      1\n",
      "3  Basically there's a family where a little boy ...  negative      0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1\n"
     ]
    }
   ],
   "source": [
    "# Преобразование меток в числовой формат\n",
    "data['label'] = data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc263d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "5000\n",
      "5000\n",
      "40000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# Разделение на обучающую и тестовую   и валидационную выборки\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['review'].tolist(),\n",
    "    data['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "print(len(train_texts))\n",
    "print(len(test_texts))\n",
    "print(len(val_texts))\n",
    "\n",
    "print(len(train_labels))\n",
    "print(len(test_labels))\n",
    "print(len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20c21b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Создание устройства, на котором будут выполняться задачи, преимущество графическому процессору.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b911bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Создание кастомной модели из стандартной берт модели. Ее не смог использовать на обучении. Ошибка tuple out of index.\n",
    "class CustomBERTModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_labels):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=num_labels)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Keep only the classification head trainable\n",
    "        for param in self.bert.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.fc = torch.nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.dropout(output(1))  # Applying dropout\n",
    "        logits = self.fc(pooled_output)  # Adding a fully connected layer\n",
    "        return logits\n",
    "\n",
    "\n",
    "#Создание обычной модели из берт .\n",
    "model_usual = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "# Замораживание весов общей модели.\n",
    "for param in model_usual.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "# Оставление размороженными только выходные слои классификатора.\n",
    "for param in model_usual.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "model_usual.to(device)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a9b1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создание класса датасета для даталоадера.  В методе гетитем значения превращаются в тензоры, \n",
    "# т.к именно тензоры требует энкодер.\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f17100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [56:54<?, ?it/s]\n",
      "  0%|          | 0/2500 [54:21<?, ?it/s]\n",
      "  0%|          | 0/2500 [26:27<?, ?it/s]\n",
      "  0%|          | 0/2500 [23:07<?, ?it/s]\n",
      "  0%|          | 0/2500 [23:30<?, ?it/s]\n",
      "  0%|          | 0/2500 [22:07<?, ?it/s]\n",
      "  0%|          | 0/2500 [21:17<?, ?it/s]\n",
      "  0%|          | 0/2500 [20:31<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "max_length=512  # Максимальная длина токена\n",
    "# Инициализация токенизатора BERT\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# Токенизация данных\n",
    "train_encodings = tokenizer(train_texts, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings = tokenizer(test_texts, return_tensors='pt',truncation=True, padding=True, max_length=max_length)\n",
    "val_encodings = tokenizer(val_texts, return_tensors='pt',truncation=True, padding=True, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b757111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Антон\\AppData\\Local\\Temp\\ipykernel_5976\\3800549983.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2065,  2017,  2066,  2010,  2265,  2017,  2453,  2022,  1037,\n",
       "          2210,  9364,  1012,  2023,  3185,  2038,  2070,  2200,  6057,  5312,\n",
       "          1998,  1996, 11680,  2024,  3492,  5377,  2021,  3904,  2024,  2200,\n",
       "         13432,  2030,  2004,  6057,  2004,  1996,  2477,  2006,  1996,  2265,\n",
       "          1012,  1996,  2927,  5537,  2003,  2428,  2428, 10021,  1998,  6057,\n",
       "          1010,  1998,  1037,  2307,  2707,  1012,  2748,   999,  8945,  8609,\n",
       "          2515,  2191,  1037, 12081,  3311,  1012,  1026,  7987,  1013,  1028,\n",
       "          1026,  7987,  1013,  1028,  2065,  2017,  2024,  1037,  5470,  2059,\n",
       "          3422,  2009,   999,  2065,  2017,  2123,  1005,  1056,  2113,  2032,\n",
       "          2030,  2123,  1005,  1056,  2066,  2032,  2059,  2123,  1005,  1056,\n",
       "          8572,  1012,  1020,  1012,  1019,  1013,  2184,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание трех токенизированных датасетов. Проверка формата.\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "\n",
    "val_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28dc2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 3\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1bf837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]C:\\Users\\Антон\\AppData\\Local\\Temp\\ipykernel_5976\\3800549983.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch number 100\n",
      "loss tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "batch number 200\n",
      "loss tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "batch number 300\n",
      "loss tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "batch number 400\n",
      "loss tensor(0.7392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "batch number 500\n",
      "loss tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "batch number 600\n",
      "loss tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m         loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_usual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#train_model(model, train_dataloader, num_epochs)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[49], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, num_epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 27\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_number\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch number\u001b[39m\u001b[38;5;124m\"\u001b[39m,batch_number)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "from tqdm import tqdm\n",
    "# Оптимизатор и функция потерь\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer = AdamW(model_usual.parameters(), lr=learning_rate)\n",
    "\n",
    "# Функция обучения\n",
    "def train_model(model, train_dataloader, num_epochs):\n",
    "    model.train()\n",
    "    metric = BinaryAccuracy(threshold=0.5)\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "        total_loss = 0\n",
    "        batch_number = 0\n",
    "        for batch in train_dataloader:\n",
    "            #print(\"batch number\",batch_number)\n",
    "            batch_number+=1\n",
    "            input_ids, attention_mask, labels = batch[\"token_type_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            #metric.update(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if batch_number%100==0:\n",
    "                print(\"batch number\",batch_number)\n",
    "                #print(\"accuracy\",metric)\n",
    "                print(\"loss\",loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader):.4f}\")\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Обучение модели\n",
    "\n",
    "train_model(model_usual, train_dataloader, num_epochs)\n",
    "#train_model(model, train_dataloader, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f222a3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
